<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>text_embeddings.base API documentation</title>
<meta name="description" content="base covers all the base classes, functions for other embedding based tokenizers." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>text_embeddings.base</code></h1>
</header>
<section id="section-intro">
<p>base covers all the base classes, functions for other embedding based tokenizers.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date    : 2021-04-22 20:43:06
# @Author  : Chenghao Mou (mouchenghao@gmail.com)

&#34;&#34;&#34;base covers all the base classes, functions for other embedding based tokenizers.&#34;&#34;&#34;

import abc
from typing import List, Optional, Union, Dict
from itertools import zip_longest

import numpy as np
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy, TruncationStrategy, TensorType, BatchEncoding, EncodedInput, is_torch_available, to_py_obj, TextInput

def is_torch(x) -&gt; bool: # pragma: no
    &#34;&#34;&#34;
    Helper function to check whether the input is a torch tensor.

    Parameters
    ----------
    x : [type]
        Input data

    Returns
    -------
    bool
        Boolean value indicating whether the input is a torch tensor
    &#34;&#34;&#34;
    import torch
    return isinstance(x, torch.Tensor)

class EmbeddingTokenizer(PreTrainedTokenizerBase):
    &#34;&#34;&#34;
    Embedding based tokenizer. It assumes each token is mapped to a tensor instead of an index number.
    This implementation borrows most implementation from huggingface&#39;s transformers library.

    Parameters
    ----------
    model_input_names : Optional[List[str]], optional
        Required model input names, by default None
    special_tokens : Optional[Dict[str, np.ndarray]], optional
        Required model special tokens, by default None
    max_length : Optional[int], optional
        Maximum sequence length supported by the model, by default 2048
    &#34;&#34;&#34;

    def __init__(
        self,
        model_input_names: Optional[List[str]] = None,
        special_tokens: Optional[Dict[str, np.ndarray]] = None,
        max_length: Optional[int] = 2048,
    ):
        self.model_input_names = model_input_names
        self.special_tokens = special_tokens
        self.max_length = max_length

    @abc.abstractmethod
    def text2embeddings(self, text: str) -&gt; np.ndarray:
        raise NotImplementedError(&#39;This function is not implemented&#39;)

    def __call__(
        self,
        text: Union[TextInput, List[TextInput]],
        text_pair: Optional[Union[TextInput, List[TextInput]]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        **kwargs,
    ) -&gt; BatchEncoding:
        &#34;&#34;&#34;
        Tokenize the text into a sequence of image blocks.

        Parameters
        ----------
        text : Union[TextInput, List[TextInput]]
            A single text or a list of text
        text_pair : Optional[Union[TextInput, List[TextInput]]], optional
            A single text or a list of text, by default None
        add_special_tokens : bool, optional
            Whether to add special tokens to the data, by default True
        padding : Union[bool, str, PaddingStrategy], optional
            The padding strategy, by default False
        truncation : Union[bool, str, TruncationStrategy], optional
            The truncation strategy, by default False
        max_length : Optional[int], optional
            Maximum sequence length, overriding the class variable, by default None
        pad_to_multiple_of : Optional[int], optional
            Padding parameters, by default None
        return_tensors : Optional[Union[str, TensorType]], optional
            Return tensors in `pt`, &#39;tf&#39; or &#39;np&#39;, by default None
        return_token_type_ids : Optional[bool], optional
            Return token type ids, by default None
        return_attention_mask : Optional[bool], optional
            Return attention mask, by default None
        return_overflowing_tokens : bool, optional
            Return overflowing tokens, by default False
        return_special_tokens_mask : bool, optional
            Return special token mask, by default False
        return_length : bool, optional
            Return length, by default False

        Returns
        -------
        BatchEncoding
            A BatchEncoding object
        &#34;&#34;&#34;
        if self.special_tokens is None:
            self.special_tokens = {
                &#34;CLS&#34;: self.text2embeddings(&#34;[CLS]&#34;),
                &#34;SEP&#34;: self.text2embeddings(&#34;[SEP]&#34;),
            }

        if add_special_tokens and text_pair:
            actual_max_length = self.max_length - len(self.special_tokens[&#34;SEP&#34;]) * 2 - len(self.special_tokens[&#34;CLS&#34;])
        else:
            actual_max_length = self.max_length

        batch_outputs = {}
        text = text if isinstance(text, list) else [text]
        text_pair = text_pair if isinstance(text_pair, list) else [text_pair]

        if isinstance(padding, str):
            padding = PaddingStrategy(padding)
        
        if isinstance(truncation, str):
            truncation = TruncationStrategy(truncation)

        for first_text, second_text in zip_longest(text, text_pair, fillvalue=None):
            
            first_embeddings = self.text2embeddings(first_text)
            second_embeddings = self.text2embeddings(second_text)

            outputs = self.prepare_for_model(
                first_embeddings,
                second_embeddings,
                add_special_tokens=add_special_tokens,
                padding=PaddingStrategy.DO_NOT_PAD,  # we pad in batch afterward
                truncation=truncation,
                max_length=max_length or actual_max_length,
                pad_to_multiple_of=None,  # we pad in batch afterward
                return_attention_mask=False,  # we pad in batch afterward
                return_token_type_ids=return_token_type_ids,
                return_overflowing_tokens=return_overflowing_tokens,
                return_special_tokens_mask=return_special_tokens_mask,
                return_length=return_length,
                return_tensors=None,  # We convert the whole batch to tensors at the end
                prepend_batch_axis=False,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        batch_outputs = self.pad(
            batch_outputs,
            padding=padding,
            max_length=max_length or actual_max_length,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)

        return batch_outputs

    def build_inputs_with_special_tokens(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return token_ids_0
        
        return np.concatenate(
            [
                self.special_tokens[&#34;CLS&#34;],
                token_ids_0,
                self.special_tokens[&#34;SEP&#34;],
                token_ids_1,
                self.special_tokens[&#34;SEP&#34;],
            ],
            axis=0
        )

    def prepare_for_model(
        self,
        ids: List[int],
        pair_ids: Optional[List[int]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        prepend_batch_axis: bool = False,
        **kwargs
    ):

        pair = bool(pair_ids is not None)
        len_ids = len(ids)
        len_pair_ids = len(pair_ids) if pair else 0
        if return_token_type_ids and not add_special_tokens:
            raise ValueError(
                &#34;Asking to return token_type_ids while setting add_special_tokens to False &#34;
                &#34;results in an undefined behavior. Please set add_special_tokens to True or &#34;
                &#34;set return_token_type_ids to None.&#34;
            )

        # Load from model defaults
        if return_token_type_ids is None:
            return_token_type_ids = &#34;token_type_ids&#34; in self.model_input_names
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        encoded_inputs = {}

        # Compute the total size of the returned encodings
        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)

        # Truncation: Handle max sequence length
        overflowing_tokens = []
        if truncation != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len &gt; max_length:
            ids, pair_ids, overflowing_tokens = self.truncate_sequences(
                ids,
                pair_ids=pair_ids,
                num_tokens_to_remove=total_len - max_length,
                truncation_strategy=truncation,
                stride=stride,
            )

        if return_overflowing_tokens:
            encoded_inputs[&#34;overflowing_tokens&#34;] = overflowing_tokens
            encoded_inputs[&#34;num_truncated_tokens&#34;] = total_len - max_length

        # Add special tokens
        if add_special_tokens:
            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)
            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)
        else:
            sequence = np.concatenate([ids, pair_ids], axis=0) if pair is True else ids
            token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])
        
        # Build output dictionary
        encoded_inputs[&#34;input_ids&#34;] = sequence

        if return_token_type_ids:
            encoded_inputs[&#34;token_type_ids&#34;] = token_type_ids
        if return_special_tokens_mask:
            if add_special_tokens:
                encoded_inputs[&#34;special_tokens_mask&#34;] = self.get_special_tokens_mask(ids, pair_ids)
            else:
                encoded_inputs[&#34;special_tokens_mask&#34;] = [0] * len(sequence)

        # Padding
        if padding != PaddingStrategy.DO_NOT_PAD or return_attention_mask:
            encoded_inputs = self.pad(
                encoded_inputs,
                max_length=max_length,
                padding=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

        if return_length:
            encoded_inputs[&#34;length&#34;] = len(encoded_inputs[&#34;input_ids&#34;])

        batch_outputs = BatchEncoding(
            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis
        )
        
        return batch_outputs
    
    def num_special_tokens_to_add(self, pair: bool = False) -&gt; int:
        return 0 if not pair else 3
    
    def get_special_tokens_mask(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return [0 for _ in token_ids_0]
        return [1 for _ in self.special_tokens[&#34;CLS&#34;]] + [0 for _ in token_ids_0] + [1 for _ in self.special_tokens[&#34;SEP&#34;]] + [0 for _ in token_ids_1] + [1 for _ in self.special_tokens[&#34;SEP&#34;]]
    
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:

        if token_ids_1 is None:
            return len(token_ids_0) * [0]
        return [0]*len(self.special_tokens[&#34;CLS&#34;]) + [0] * len(token_ids_0) + [0]*len(self.special_tokens[&#34;SEP&#34;]) + [1] * len(token_ids_1) + [0]*len(self.special_tokens[&#34;SEP&#34;])

    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            List[BatchEncoding],
            Dict[str, EncodedInput],
            Dict[str, List[EncodedInput]],
            List[Dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = True,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
    ) -&gt; BatchEncoding:

        # If we have a list of dicts, let&#39;s convert it in a dict of lists
        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):
            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}

        # The model&#39;s main input name, usually `input_ids`, has be passed for padding
        if self.model_input_names[0] not in encoded_inputs:
            raise ValueError(
                &#34;You should supply an encoding or a list of encodings to this method&#34;
                f&#34;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&#34;
            )

        required_input = encoded_inputs[self.model_input_names[0]]

        if required_input is None:
            if return_attention_mask:
                encoded_inputs[&#34;attention_mask&#34;] = []
            return encoded_inputs

        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects
        # and rebuild them afterwards if no return_tensors is specified
        # Note that we lose the specific device the tensor may be on for PyTorch

        first_element = required_input[0]
        if isinstance(first_element, (list, tuple)):
            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
            index = 0
            while len(required_input[index]) == 0:
                index += 1
            if index &lt; len(required_input):
                first_element = required_input[index][0]
        # At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.
        if not isinstance(first_element, (int, list, tuple)):
            if is_torch_available() and is_torch(first_element):
                return_tensors = &#34;pt&#34; if return_tensors is None else return_tensors
            elif isinstance(first_element, np.ndarray):
                return_tensors = &#34;np&#34; if return_tensors is None else return_tensors
            else:
                raise ValueError(
                    f&#34;type of {first_element} unknown: {type(first_element)}. &#34;
                    f&#34;Should be one of a python, numpy or pytorch object.&#34;
                )

            for key, value in encoded_inputs.items():
                encoded_inputs[key] = to_py_obj(value)
        
        required_input = encoded_inputs[self.model_input_names[0]]
        if required_input and not isinstance(required_input[0], (list, tuple)):
            encoded_inputs = self._pad(
                encoded_inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )
            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)

        batch_size = len(required_input)
        assert all(
            len(v) == batch_size for v in encoded_inputs.values()
        ), &#34;Some items in the output dictionary have a different batch size than others.&#34;

        if padding == PaddingStrategy.LONGEST:
            max_length = max(len(inputs) for inputs in required_input)
            padding = PaddingStrategy.MAX_LENGTH

        batch_outputs = {}
        for i in range(batch_size):
            inputs = dict((k, v[i]) for k, v in encoded_inputs.items())
            outputs = self._pad(
                inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        return BatchEncoding(batch_outputs, tensor_type=return_tensors)
    
    def create_padding_token_embedding(self, input_embeddings=None):
        raise NotImplementedError(&#39;This function is not implemented&#39;)

    def _pad(
        self,
        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
        max_length: Optional[int] = None,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
    ) -&gt; dict:

        # Load from model defaults
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        required_input = encoded_inputs[self.model_input_names[0]]
        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length

        if needs_to_be_padded:
            difference = max_length - len(required_input)
            if &#34;token_type_ids&#34; in encoded_inputs and isinstance(encoded_inputs[&#34;token_type_ids&#34;], int):
                encoded_inputs[&#34;token_type_ids&#34;] = [encoded_inputs[&#34;token_type_ids&#34;]]
            if self.padding_side == &#34;right&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input) + [0] * difference
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = (
                        encoded_inputs[&#34;token_type_ids&#34;] + [1] * difference
                    )
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = encoded_inputs[&#34;special_tokens_mask&#34;] + [1] * difference
                
                encoded_inputs[self.model_input_names[0]] = required_input + [self.create_padding_token_embedding(input_embeddings=required_input)] * difference
            elif self.padding_side == &#34;left&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [0] * difference + [1] * len(required_input)
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = [0] * difference + encoded_inputs[
                        &#34;token_type_ids&#34;
                    ]
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = [1] * difference + encoded_inputs[&#34;special_tokens_mask&#34;]
                encoded_inputs[self.model_input_names[0]] = [self.create_padding_token_embedding(input_embeddings=required_input)] * difference + required_input
            else:
                raise ValueError(&#34;Invalid padding strategy:&#34; + str(self.padding_side))
        elif return_attention_mask and &#34;attention_mask&#34; not in encoded_inputs:
            encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input)

        return encoded_inputs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="text_embeddings.base.is_torch"><code class="name flex">
<span>def <span class="ident">is_torch</span></span>(<span>x) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to check whether the input is a torch tensor.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>[type]</code></dt>
<dd>Input data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Boolean value indicating whether the input is a torch tensor</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_torch(x) -&gt; bool: # pragma: no
    &#34;&#34;&#34;
    Helper function to check whether the input is a torch tensor.

    Parameters
    ----------
    x : [type]
        Input data

    Returns
    -------
    bool
        Boolean value indicating whether the input is a torch tensor
    &#34;&#34;&#34;
    import torch
    return isinstance(x, torch.Tensor)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="text_embeddings.base.EmbeddingTokenizer"><code class="flex name class">
<span>class <span class="ident">EmbeddingTokenizer</span></span>
<span>(</span><span>model_input_names: Union[List[str], NoneType] = None, special_tokens: Union[Dict[str, numpy.ndarray], NoneType] = None, max_length: Union[int, NoneType] = 2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Embedding based tokenizer. It assumes each token is mapped to a tensor instead of an index number.
This implementation borrows most implementation from huggingface's transformers library.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_input_names</code></strong> :&ensp;<code>Optional[List[str]]</code>, optional</dt>
<dd>Required model input names, by default None</dd>
<dt><strong><code>special_tokens</code></strong> :&ensp;<code>Optional[Dict[str, np.ndarray]]</code>, optional</dt>
<dd>Required model special tokens, by default None</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Maximum sequence length supported by the model, by default 2048</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmbeddingTokenizer(PreTrainedTokenizerBase):
    &#34;&#34;&#34;
    Embedding based tokenizer. It assumes each token is mapped to a tensor instead of an index number.
    This implementation borrows most implementation from huggingface&#39;s transformers library.

    Parameters
    ----------
    model_input_names : Optional[List[str]], optional
        Required model input names, by default None
    special_tokens : Optional[Dict[str, np.ndarray]], optional
        Required model special tokens, by default None
    max_length : Optional[int], optional
        Maximum sequence length supported by the model, by default 2048
    &#34;&#34;&#34;

    def __init__(
        self,
        model_input_names: Optional[List[str]] = None,
        special_tokens: Optional[Dict[str, np.ndarray]] = None,
        max_length: Optional[int] = 2048,
    ):
        self.model_input_names = model_input_names
        self.special_tokens = special_tokens
        self.max_length = max_length

    @abc.abstractmethod
    def text2embeddings(self, text: str) -&gt; np.ndarray:
        raise NotImplementedError(&#39;This function is not implemented&#39;)

    def __call__(
        self,
        text: Union[TextInput, List[TextInput]],
        text_pair: Optional[Union[TextInput, List[TextInput]]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        **kwargs,
    ) -&gt; BatchEncoding:
        &#34;&#34;&#34;
        Tokenize the text into a sequence of image blocks.

        Parameters
        ----------
        text : Union[TextInput, List[TextInput]]
            A single text or a list of text
        text_pair : Optional[Union[TextInput, List[TextInput]]], optional
            A single text or a list of text, by default None
        add_special_tokens : bool, optional
            Whether to add special tokens to the data, by default True
        padding : Union[bool, str, PaddingStrategy], optional
            The padding strategy, by default False
        truncation : Union[bool, str, TruncationStrategy], optional
            The truncation strategy, by default False
        max_length : Optional[int], optional
            Maximum sequence length, overriding the class variable, by default None
        pad_to_multiple_of : Optional[int], optional
            Padding parameters, by default None
        return_tensors : Optional[Union[str, TensorType]], optional
            Return tensors in `pt`, &#39;tf&#39; or &#39;np&#39;, by default None
        return_token_type_ids : Optional[bool], optional
            Return token type ids, by default None
        return_attention_mask : Optional[bool], optional
            Return attention mask, by default None
        return_overflowing_tokens : bool, optional
            Return overflowing tokens, by default False
        return_special_tokens_mask : bool, optional
            Return special token mask, by default False
        return_length : bool, optional
            Return length, by default False

        Returns
        -------
        BatchEncoding
            A BatchEncoding object
        &#34;&#34;&#34;
        if self.special_tokens is None:
            self.special_tokens = {
                &#34;CLS&#34;: self.text2embeddings(&#34;[CLS]&#34;),
                &#34;SEP&#34;: self.text2embeddings(&#34;[SEP]&#34;),
            }

        if add_special_tokens and text_pair:
            actual_max_length = self.max_length - len(self.special_tokens[&#34;SEP&#34;]) * 2 - len(self.special_tokens[&#34;CLS&#34;])
        else:
            actual_max_length = self.max_length

        batch_outputs = {}
        text = text if isinstance(text, list) else [text]
        text_pair = text_pair if isinstance(text_pair, list) else [text_pair]

        if isinstance(padding, str):
            padding = PaddingStrategy(padding)
        
        if isinstance(truncation, str):
            truncation = TruncationStrategy(truncation)

        for first_text, second_text in zip_longest(text, text_pair, fillvalue=None):
            
            first_embeddings = self.text2embeddings(first_text)
            second_embeddings = self.text2embeddings(second_text)

            outputs = self.prepare_for_model(
                first_embeddings,
                second_embeddings,
                add_special_tokens=add_special_tokens,
                padding=PaddingStrategy.DO_NOT_PAD,  # we pad in batch afterward
                truncation=truncation,
                max_length=max_length or actual_max_length,
                pad_to_multiple_of=None,  # we pad in batch afterward
                return_attention_mask=False,  # we pad in batch afterward
                return_token_type_ids=return_token_type_ids,
                return_overflowing_tokens=return_overflowing_tokens,
                return_special_tokens_mask=return_special_tokens_mask,
                return_length=return_length,
                return_tensors=None,  # We convert the whole batch to tensors at the end
                prepend_batch_axis=False,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        batch_outputs = self.pad(
            batch_outputs,
            padding=padding,
            max_length=max_length or actual_max_length,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)

        return batch_outputs

    def build_inputs_with_special_tokens(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return token_ids_0
        
        return np.concatenate(
            [
                self.special_tokens[&#34;CLS&#34;],
                token_ids_0,
                self.special_tokens[&#34;SEP&#34;],
                token_ids_1,
                self.special_tokens[&#34;SEP&#34;],
            ],
            axis=0
        )

    def prepare_for_model(
        self,
        ids: List[int],
        pair_ids: Optional[List[int]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        prepend_batch_axis: bool = False,
        **kwargs
    ):

        pair = bool(pair_ids is not None)
        len_ids = len(ids)
        len_pair_ids = len(pair_ids) if pair else 0
        if return_token_type_ids and not add_special_tokens:
            raise ValueError(
                &#34;Asking to return token_type_ids while setting add_special_tokens to False &#34;
                &#34;results in an undefined behavior. Please set add_special_tokens to True or &#34;
                &#34;set return_token_type_ids to None.&#34;
            )

        # Load from model defaults
        if return_token_type_ids is None:
            return_token_type_ids = &#34;token_type_ids&#34; in self.model_input_names
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        encoded_inputs = {}

        # Compute the total size of the returned encodings
        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)

        # Truncation: Handle max sequence length
        overflowing_tokens = []
        if truncation != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len &gt; max_length:
            ids, pair_ids, overflowing_tokens = self.truncate_sequences(
                ids,
                pair_ids=pair_ids,
                num_tokens_to_remove=total_len - max_length,
                truncation_strategy=truncation,
                stride=stride,
            )

        if return_overflowing_tokens:
            encoded_inputs[&#34;overflowing_tokens&#34;] = overflowing_tokens
            encoded_inputs[&#34;num_truncated_tokens&#34;] = total_len - max_length

        # Add special tokens
        if add_special_tokens:
            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)
            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)
        else:
            sequence = np.concatenate([ids, pair_ids], axis=0) if pair is True else ids
            token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])
        
        # Build output dictionary
        encoded_inputs[&#34;input_ids&#34;] = sequence

        if return_token_type_ids:
            encoded_inputs[&#34;token_type_ids&#34;] = token_type_ids
        if return_special_tokens_mask:
            if add_special_tokens:
                encoded_inputs[&#34;special_tokens_mask&#34;] = self.get_special_tokens_mask(ids, pair_ids)
            else:
                encoded_inputs[&#34;special_tokens_mask&#34;] = [0] * len(sequence)

        # Padding
        if padding != PaddingStrategy.DO_NOT_PAD or return_attention_mask:
            encoded_inputs = self.pad(
                encoded_inputs,
                max_length=max_length,
                padding=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

        if return_length:
            encoded_inputs[&#34;length&#34;] = len(encoded_inputs[&#34;input_ids&#34;])

        batch_outputs = BatchEncoding(
            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis
        )
        
        return batch_outputs
    
    def num_special_tokens_to_add(self, pair: bool = False) -&gt; int:
        return 0 if not pair else 3
    
    def get_special_tokens_mask(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return [0 for _ in token_ids_0]
        return [1 for _ in self.special_tokens[&#34;CLS&#34;]] + [0 for _ in token_ids_0] + [1 for _ in self.special_tokens[&#34;SEP&#34;]] + [0 for _ in token_ids_1] + [1 for _ in self.special_tokens[&#34;SEP&#34;]]
    
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:

        if token_ids_1 is None:
            return len(token_ids_0) * [0]
        return [0]*len(self.special_tokens[&#34;CLS&#34;]) + [0] * len(token_ids_0) + [0]*len(self.special_tokens[&#34;SEP&#34;]) + [1] * len(token_ids_1) + [0]*len(self.special_tokens[&#34;SEP&#34;])

    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            List[BatchEncoding],
            Dict[str, EncodedInput],
            Dict[str, List[EncodedInput]],
            List[Dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = True,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
    ) -&gt; BatchEncoding:

        # If we have a list of dicts, let&#39;s convert it in a dict of lists
        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):
            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}

        # The model&#39;s main input name, usually `input_ids`, has be passed for padding
        if self.model_input_names[0] not in encoded_inputs:
            raise ValueError(
                &#34;You should supply an encoding or a list of encodings to this method&#34;
                f&#34;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&#34;
            )

        required_input = encoded_inputs[self.model_input_names[0]]

        if required_input is None:
            if return_attention_mask:
                encoded_inputs[&#34;attention_mask&#34;] = []
            return encoded_inputs

        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects
        # and rebuild them afterwards if no return_tensors is specified
        # Note that we lose the specific device the tensor may be on for PyTorch

        first_element = required_input[0]
        if isinstance(first_element, (list, tuple)):
            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
            index = 0
            while len(required_input[index]) == 0:
                index += 1
            if index &lt; len(required_input):
                first_element = required_input[index][0]
        # At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.
        if not isinstance(first_element, (int, list, tuple)):
            if is_torch_available() and is_torch(first_element):
                return_tensors = &#34;pt&#34; if return_tensors is None else return_tensors
            elif isinstance(first_element, np.ndarray):
                return_tensors = &#34;np&#34; if return_tensors is None else return_tensors
            else:
                raise ValueError(
                    f&#34;type of {first_element} unknown: {type(first_element)}. &#34;
                    f&#34;Should be one of a python, numpy or pytorch object.&#34;
                )

            for key, value in encoded_inputs.items():
                encoded_inputs[key] = to_py_obj(value)
        
        required_input = encoded_inputs[self.model_input_names[0]]
        if required_input and not isinstance(required_input[0], (list, tuple)):
            encoded_inputs = self._pad(
                encoded_inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )
            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)

        batch_size = len(required_input)
        assert all(
            len(v) == batch_size for v in encoded_inputs.values()
        ), &#34;Some items in the output dictionary have a different batch size than others.&#34;

        if padding == PaddingStrategy.LONGEST:
            max_length = max(len(inputs) for inputs in required_input)
            padding = PaddingStrategy.MAX_LENGTH

        batch_outputs = {}
        for i in range(batch_size):
            inputs = dict((k, v[i]) for k, v in encoded_inputs.items())
            outputs = self._pad(
                inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        return BatchEncoding(batch_outputs, tensor_type=return_tensors)
    
    def create_padding_token_embedding(self, input_embeddings=None):
        raise NotImplementedError(&#39;This function is not implemented&#39;)

    def _pad(
        self,
        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
        max_length: Optional[int] = None,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
    ) -&gt; dict:

        # Load from model defaults
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        required_input = encoded_inputs[self.model_input_names[0]]
        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length

        if needs_to_be_padded:
            difference = max_length - len(required_input)
            if &#34;token_type_ids&#34; in encoded_inputs and isinstance(encoded_inputs[&#34;token_type_ids&#34;], int):
                encoded_inputs[&#34;token_type_ids&#34;] = [encoded_inputs[&#34;token_type_ids&#34;]]
            if self.padding_side == &#34;right&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input) + [0] * difference
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = (
                        encoded_inputs[&#34;token_type_ids&#34;] + [1] * difference
                    )
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = encoded_inputs[&#34;special_tokens_mask&#34;] + [1] * difference
                
                encoded_inputs[self.model_input_names[0]] = required_input + [self.create_padding_token_embedding(input_embeddings=required_input)] * difference
            elif self.padding_side == &#34;left&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [0] * difference + [1] * len(required_input)
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = [0] * difference + encoded_inputs[
                        &#34;token_type_ids&#34;
                    ]
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = [1] * difference + encoded_inputs[&#34;special_tokens_mask&#34;]
                encoded_inputs[self.model_input_names[0]] = [self.create_padding_token_embedding(input_embeddings=required_input)] * difference + required_input
            else:
                raise ValueError(&#34;Invalid padding strategy:&#34; + str(self.padding_side))
        elif return_attention_mask and &#34;attention_mask&#34; not in encoded_inputs:
            encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input)

        return encoded_inputs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.tokenization_utils_base.PreTrainedTokenizerBase</li>
<li>transformers.tokenization_utils_base.SpecialTokensMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="text_embeddings.byte.byt5.BYT5Tokenizer" href="../byte/byt5.html#text_embeddings.byte.byt5.BYT5Tokenizer">BYT5Tokenizer</a></li>
<li><a title="text_embeddings.hash.canine.CANINETokenizer" href="../hash/canine.html#text_embeddings.hash.canine.CANINETokenizer">CANINETokenizer</a></li>
<li><a title="text_embeddings.hash.pqrnn.PQRNNTokenizer" href="../hash/pqrnn.html#text_embeddings.hash.pqrnn.PQRNNTokenizer">PQRNNTokenizer</a></li>
<li><a title="text_embeddings.visual.vtr.VTRTokenizer" href="../visual/vtr.html#text_embeddings.visual.vtr.VTRTokenizer">VTRTokenizer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="text_embeddings.base.EmbeddingTokenizer.max_model_input_sizes"><code class="name">var <span class="ident">max_model_input_sizes</span> : Dict[str, Union[int, NoneType]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.model_input_names"><code class="name">var <span class="ident">model_input_names</span> : List[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.padding_side"><code class="name">var <span class="ident">padding_side</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.pretrained_init_configuration"><code class="name">var <span class="ident">pretrained_init_configuration</span> : Dict[str, Dict[str, Any]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.pretrained_vocab_files_map"><code class="name">var <span class="ident">pretrained_vocab_files_map</span> : Dict[str, Dict[str, str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.vocab_files_names"><code class="name">var <span class="ident">vocab_files_names</span> : Dict[str, str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="text_embeddings.base.EmbeddingTokenizer.build_inputs_with_special_tokens"><code class="name flex">
<span>def <span class="ident">build_inputs_with_special_tokens</span></span>(<span>self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.</p>
<p>This implementation does not add special tokens and this method should be overridden in a subclass.</p>
<h2 id="args">Args</h2>
<p>token_ids_0 (:obj:<code>List[int]</code>): The first tokenized sequence.
token_ids_1 (:obj:<code>List[int]</code>, <code>optional</code>): The second tokenized sequence.</p>
<h2 id="returns">Returns</h2>
<p>:obj:<code>List[int]</code>: The model input with special tokens.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_inputs_with_special_tokens(
    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -&gt; List[int]:
    if token_ids_1 is None:
        return token_ids_0
    
    return np.concatenate(
        [
            self.special_tokens[&#34;CLS&#34;],
            token_ids_0,
            self.special_tokens[&#34;SEP&#34;],
            token_ids_1,
            self.special_tokens[&#34;SEP&#34;],
        ],
        axis=0
    )</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.create_padding_token_embedding"><code class="name flex">
<span>def <span class="ident">create_padding_token_embedding</span></span>(<span>self, input_embeddings=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_padding_token_embedding(self, input_embeddings=None):
    raise NotImplementedError(&#39;This function is not implemented&#39;)</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.create_token_type_ids_from_sequences"><code class="name flex">
<span>def <span class="ident">create_token_type_ids_from_sequences</span></span>(<span>self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Create the token type IDs corresponding to the sequences passed. <code>What are token type IDs?
&lt;../glossary.html#token-type-ids&gt;</code>__</p>
<p>Should be overridden in a subclass if the model has a special way of building those.</p>
<h2 id="args">Args</h2>
<p>token_ids_0 (:obj:<code>List[int]</code>): The first tokenized sequence.
token_ids_1 (:obj:<code>List[int]</code>, <code>optional</code>): The second tokenized sequence.</p>
<h2 id="returns">Returns</h2>
<p>:obj:<code>List[int]</code>: The token type ids.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_token_type_ids_from_sequences(
    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -&gt; List[int]:

    if token_ids_1 is None:
        return len(token_ids_0) * [0]
    return [0]*len(self.special_tokens[&#34;CLS&#34;]) + [0] * len(token_ids_0) + [0]*len(self.special_tokens[&#34;SEP&#34;]) + [1] * len(token_ids_1) + [0]*len(self.special_tokens[&#34;SEP&#34;])</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.get_special_tokens_mask"><code class="name flex">
<span>def <span class="ident">get_special_tokens_mask</span></span>(<span>self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> or <code>encode_plus</code> methods.</p>
<h2 id="args">Args</h2>
<p>token_ids_0 (:obj:<code>List[int]</code>):
List of ids of the first sequence.
token_ids_1 (:obj:<code>List[int]</code>, <code>optional</code>):
List of ids of the second sequence.
already_has_special_tokens (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not the token list is already formatted with special tokens for the model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>A list</code> of <code>integers in the range [0, 1]</code></dt>
<dd>1 for a special token, 0 for a sequence token.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_special_tokens_mask(
    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -&gt; List[int]:
    if token_ids_1 is None:
        return [0 for _ in token_ids_0]
    return [1 for _ in self.special_tokens[&#34;CLS&#34;]] + [0 for _ in token_ids_0] + [1 for _ in self.special_tokens[&#34;SEP&#34;]] + [0 for _ in token_ids_1] + [1 for _ in self.special_tokens[&#34;SEP&#34;]]</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.num_special_tokens_to_add"><code class="name flex">
<span>def <span class="ident">num_special_tokens_to_add</span></span>(<span>self, pair: bool = False) ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def num_special_tokens_to_add(self, pair: bool = False) -&gt; int:
    return 0 if not pair else 3</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None) ‑> transformers.tokenization_utils_base.BatchEncoding</span>
</code></dt>
<dd>
<div class="desc"><p>Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
in the batch.</p>
<p>Padding side (left/right) padding token ids are defined at the tokenizer level (with <code>self.padding_side</code>,
<code>self.pad_token_id</code> and <code>self.pad_token_type_id</code>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code>encoded_inputs</code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code>return_tensors</code>. In the
case of PyTorch tensors, you will lose the specific device of your tensors however.</p>
</div>
<h2 id="args">Args</h2>
<p>encoded_inputs (:class:<code>~transformers.BatchEncoding</code>, list of :class:<code>~transformers.BatchEncoding</code>, :obj:<code>Dict[str, List[int]]</code>, :obj:<code>Dict[str, List[List[int]]</code> or :obj:<code>List[Dict[str, List[int]]]</code>):
Tokenized inputs. Can represent one input (:class:<code>~transformers.BatchEncoding</code> or :obj:<code>Dict[str,
List[int]]</code>) or a batch of tokenized inputs (list of :class:<code>~transformers.BatchEncoding&lt;code&gt;, &lt;/code&gt;Dict[str,
List[List[int]]]&lt;code&gt; or &lt;/code&gt;List[Dict[str, List[int]]]</code>) so you can use this method during preprocessing as
well as in a PyTorch Dataloader collate function.</p>
<pre><code>Instead of :obj:&lt;code&gt;List\[int]&lt;/code&gt; you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.
</code></pre>
<p>padding (:obj:<code>bool</code>, :obj:<code>str</code> or :class:<code>~transformers.file_utils.PaddingStrategy</code>, <code>optional</code>, defaults to :obj:<code>True</code>):
Select a strategy to pad the returned sequences (according to the model's padding side and padding
index) among:</p>
<pre><code>* :obj:&lt;code&gt;True&lt;/code&gt; or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a
  single sequence if provided).
* :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or to the
  maximum acceptable input length for the model if that argument is not provided.
* :obj:&lt;code&gt;False&lt;/code&gt; or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
  different lengths).
</code></pre>
<p>max_length (:obj:<code>int</code>, <code>optional</code>):
Maximum length of the returned list and optionally padding length (see above).
pad_to_multiple_of (:obj:<code>int</code>, <code>optional</code>):
If set will pad the sequence to a multiple of the provided value.</p>
<pre><code>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
&gt;= 7.5 (Volta).
</code></pre>
<p>return_attention_mask (:obj:<code>bool</code>, <code>optional</code>):
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer's default, defined by the :obj:<code>return_outputs</code> attribute.</p>
<pre><code>`What are attention masks? &lt;../glossary.html#attention-mask&gt;`__
</code></pre>
<p>return_tensors (:obj:<code>str</code> or :class:<code>~transformers.file_utils.TensorType</code>, <code>optional</code>):
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<pre><code>* :obj:`'tf'`: Return TensorFlow :obj:&lt;code&gt;tf.constant&lt;/code&gt; objects.
* :obj:`'pt'`: Return PyTorch :obj:&lt;code&gt;torch.Tensor&lt;/code&gt; objects.
* :obj:`'np'`: Return Numpy :obj:&lt;code&gt;np.ndarray&lt;/code&gt; objects.
</code></pre>
<p>verbose (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>True</code>):
Whether or not to print more information and warnings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(
    self,
    encoded_inputs: Union[
        BatchEncoding,
        List[BatchEncoding],
        Dict[str, EncodedInput],
        Dict[str, List[EncodedInput]],
        List[Dict[str, EncodedInput]],
    ],
    padding: Union[bool, str, PaddingStrategy] = True,
    max_length: Optional[int] = None,
    pad_to_multiple_of: Optional[int] = None,
    return_attention_mask: Optional[bool] = None,
    return_tensors: Optional[Union[str, TensorType]] = None,
) -&gt; BatchEncoding:

    # If we have a list of dicts, let&#39;s convert it in a dict of lists
    # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):
        encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}

    # The model&#39;s main input name, usually `input_ids`, has be passed for padding
    if self.model_input_names[0] not in encoded_inputs:
        raise ValueError(
            &#34;You should supply an encoding or a list of encodings to this method&#34;
            f&#34;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&#34;
        )

    required_input = encoded_inputs[self.model_input_names[0]]

    if required_input is None:
        if return_attention_mask:
            encoded_inputs[&#34;attention_mask&#34;] = []
        return encoded_inputs

    # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects
    # and rebuild them afterwards if no return_tensors is specified
    # Note that we lose the specific device the tensor may be on for PyTorch

    first_element = required_input[0]
    if isinstance(first_element, (list, tuple)):
        # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
        index = 0
        while len(required_input[index]) == 0:
            index += 1
        if index &lt; len(required_input):
            first_element = required_input[index][0]
    # At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.
    if not isinstance(first_element, (int, list, tuple)):
        if is_torch_available() and is_torch(first_element):
            return_tensors = &#34;pt&#34; if return_tensors is None else return_tensors
        elif isinstance(first_element, np.ndarray):
            return_tensors = &#34;np&#34; if return_tensors is None else return_tensors
        else:
            raise ValueError(
                f&#34;type of {first_element} unknown: {type(first_element)}. &#34;
                f&#34;Should be one of a python, numpy or pytorch object.&#34;
            )

        for key, value in encoded_inputs.items():
            encoded_inputs[key] = to_py_obj(value)
    
    required_input = encoded_inputs[self.model_input_names[0]]
    if required_input and not isinstance(required_input[0], (list, tuple)):
        encoded_inputs = self._pad(
            encoded_inputs,
            max_length=max_length,
            padding_strategy=padding,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )
        return BatchEncoding(encoded_inputs, tensor_type=return_tensors)

    batch_size = len(required_input)
    assert all(
        len(v) == batch_size for v in encoded_inputs.values()
    ), &#34;Some items in the output dictionary have a different batch size than others.&#34;

    if padding == PaddingStrategy.LONGEST:
        max_length = max(len(inputs) for inputs in required_input)
        padding = PaddingStrategy.MAX_LENGTH

    batch_outputs = {}
    for i in range(batch_size):
        inputs = dict((k, v[i]) for k, v in encoded_inputs.items())
        outputs = self._pad(
            inputs,
            max_length=max_length,
            padding_strategy=padding,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        for key, value in outputs.items():
            if key not in batch_outputs:
                batch_outputs[key] = []
            batch_outputs[key].append(value)

    return BatchEncoding(batch_outputs, tensor_type=return_tensors)</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.prepare_for_model"><code class="name flex">
<span>def <span class="ident">prepare_for_model</span></span>(<span>self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_length: bool = False, prepend_batch_axis: bool = False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens</p>
<h2 id="args">Args</h2>
<p>ids (:obj:<code>List[int]</code>):
Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code>tokenize</code>
and <code>convert_tokens_to_ids</code> methods.
pair_ids (:obj:<code>List[int]</code>, <code>optional</code>):
Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code>tokenize</code>
and <code>convert_tokens_to_ids</code> methods.</p>
<p>add_special_tokens (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>True</code>):
Whether or not to encode the sequences with the special tokens relative to their model.
padding (:obj:<code>bool</code>, :obj:<code>str</code> or :class:<code>~transformers.file_utils.PaddingStrategy</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Activates and controls padding. Accepts the following values:</p>
<pre><code>* :obj:&lt;code&gt;True&lt;/code&gt; or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a
  single sequence if provided).
* :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or to the
  maximum acceptable input length for the model if that argument is not provided.
* :obj:&lt;code&gt;False&lt;/code&gt; or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
  different lengths).
</code></pre>
<p>truncation (:obj:<code>bool</code>, :obj:<code>str</code> or :class:<code>~transformers.tokenization_utils_base.TruncationStrategy</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Activates and controls truncation. Accepts the following values:</p>
<pre><code>* :obj:&lt;code&gt;True&lt;/code&gt; or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument
  :obj:&lt;code&gt;max\_length&lt;/code&gt; or to the maximum acceptable input length for the model if that argument is not
  provided. This will truncate token by token, removing a token from the longest sequence in the pair
  if a pair of sequences (or a batch of pairs) is provided.
* :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or to
  the maximum acceptable input length for the model if that argument is not provided. This will only
  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
* :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or
  to the maximum acceptable input length for the model if that argument is not provided. This will only
  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
* :obj:&lt;code&gt;False&lt;/code&gt; or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with
  sequence lengths greater than the model maximum admissible input size).
</code></pre>
<p>max_length (:obj:<code>int</code>, <code>optional</code>):
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<pre><code>If left unset or set to :obj:&lt;code&gt;None&lt;/code&gt;, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.
</code></pre>
<p>stride (:obj:<code>int</code>, <code>optional</code>, defaults to 0):
If set to a number along with :obj:<code>max_length</code>, the overflowing tokens returned when
:obj:<code>return_overflowing_tokens=True</code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
is_split_into_words (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.
pad_to_multiple_of (:obj:<code>int</code>, <code>optional</code>):
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).
return_tensors (:obj:<code>str</code> or :class:<code>~transformers.file_utils.TensorType</code>, <code>optional</code>):
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<pre><code>* :obj:`'tf'`: Return TensorFlow :obj:&lt;code&gt;tf.constant&lt;/code&gt; objects.
* :obj:`'pt'`: Return PyTorch :obj:&lt;code&gt;torch.Tensor&lt;/code&gt; objects.
* :obj:`'np'`: Return Numpy :obj:&lt;code&gt;np.ndarray&lt;/code&gt; objects.
</code></pre>
<p>return_token_type_ids (:obj:<code>bool</code>, <code>optional</code>):
Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer's default, defined by the :obj:<code>return_outputs</code> attribute.</p>
<pre><code>`What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`__
</code></pre>
<p>return_attention_mask (:obj:<code>bool</code>, <code>optional</code>):
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer's default, defined by the :obj:<code>return_outputs</code> attribute.</p>
<pre><code>`What are attention masks? &lt;../glossary.html#attention-mask&gt;`__
</code></pre>
<p>return_overflowing_tokens (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not to return overflowing token sequences.
return_special_tokens_mask (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not to return special tokens mask information.
return_offsets_mapping (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not to return :obj:<code>(char_start, char_end)</code> for each token.</p>
<pre><code>This is only available on fast tokenizers inheriting from
:class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise
:obj:&lt;code&gt;NotImplementedError&lt;/code&gt;.
</code></pre>
<dl>
<dt>return_length
(:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):</dt>
<dt>Whether or not to return the lengths of the encoded inputs.</dt>
<dt>verbose (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>True</code>):</dt>
<dt>Whether or not to print more information and warnings.</dt>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>passed to the :obj:<code>self.tokenize()</code> method</dd>
</dl>
<h2 id="return">Return</h2>
<p>:class:<code>~transformers.BatchEncoding</code>: A :class:<code>~transformers.BatchEncoding</code> with the following fields:</p>
<ul>
<li><strong>input_ids</strong> &ndash; List of token ids to be fed to a model.</li>
</ul>
<p><code>What are input IDs? &lt;../glossary.html#input-ids&gt;</code>__</p>
<ul>
<li><strong>token_type_ids</strong> &ndash; List of token type ids to be fed to a model (when :obj:<code>return_token_type_ids=True</code>
or if <code>"token_type_ids"</code> is in :obj:<code>self.model_input_names</code>).</li>
</ul>
<p><code>What are token type IDs? &lt;../glossary.html#token-type-ids&gt;</code>__</p>
<ul>
<li><strong>attention_mask</strong> &ndash; List of indices specifying which tokens should be attended to by the model (when
:obj:<code>return_attention_mask=True</code> or if <code>"attention_mask"</code> is in :obj:<code>self.model_input_names</code>).</li>
</ul>
<p><code>What are attention masks? &lt;../glossary.html#attention-mask&gt;</code>__</p>
<ul>
<li><strong>overflowing_tokens</strong> &ndash; List of overflowing tokens sequences (when a :obj:<code>max_length</code> is specified and
:obj:<code>return_overflowing_tokens=True</code>).</li>
<li><strong>num_truncated_tokens</strong> &ndash; Number of tokens truncated (when a :obj:<code>max_length</code> is specified and
:obj:<code>return_overflowing_tokens=True</code>).</li>
<li><strong>special_tokens_mask</strong> &ndash; List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when :obj:<code>add_special_tokens=True</code> and :obj:<code>return_special_tokens_mask=True</code>).</li>
<li><strong>length</strong> &ndash; The length of the inputs (when :obj:<code>return_length=True</code>)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_model(
    self,
    ids: List[int],
    pair_ids: Optional[List[int]] = None,
    add_special_tokens: bool = True,
    padding: Union[bool, str, PaddingStrategy] = False,
    truncation: Union[bool, str, TruncationStrategy] = False,
    max_length: Optional[int] = None,
    stride: int = 0,
    pad_to_multiple_of: Optional[int] = None,
    return_tensors: Optional[Union[str, TensorType]] = None,
    return_token_type_ids: Optional[bool] = None,
    return_attention_mask: Optional[bool] = None,
    return_overflowing_tokens: bool = False,
    return_special_tokens_mask: bool = False,
    return_length: bool = False,
    prepend_batch_axis: bool = False,
    **kwargs
):

    pair = bool(pair_ids is not None)
    len_ids = len(ids)
    len_pair_ids = len(pair_ids) if pair else 0
    if return_token_type_ids and not add_special_tokens:
        raise ValueError(
            &#34;Asking to return token_type_ids while setting add_special_tokens to False &#34;
            &#34;results in an undefined behavior. Please set add_special_tokens to True or &#34;
            &#34;set return_token_type_ids to None.&#34;
        )

    # Load from model defaults
    if return_token_type_ids is None:
        return_token_type_ids = &#34;token_type_ids&#34; in self.model_input_names
    if return_attention_mask is None:
        return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

    encoded_inputs = {}

    # Compute the total size of the returned encodings
    total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)

    # Truncation: Handle max sequence length
    overflowing_tokens = []
    if truncation != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len &gt; max_length:
        ids, pair_ids, overflowing_tokens = self.truncate_sequences(
            ids,
            pair_ids=pair_ids,
            num_tokens_to_remove=total_len - max_length,
            truncation_strategy=truncation,
            stride=stride,
        )

    if return_overflowing_tokens:
        encoded_inputs[&#34;overflowing_tokens&#34;] = overflowing_tokens
        encoded_inputs[&#34;num_truncated_tokens&#34;] = total_len - max_length

    # Add special tokens
    if add_special_tokens:
        sequence = self.build_inputs_with_special_tokens(ids, pair_ids)
        token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)
    else:
        sequence = np.concatenate([ids, pair_ids], axis=0) if pair is True else ids
        token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])
    
    # Build output dictionary
    encoded_inputs[&#34;input_ids&#34;] = sequence

    if return_token_type_ids:
        encoded_inputs[&#34;token_type_ids&#34;] = token_type_ids
    if return_special_tokens_mask:
        if add_special_tokens:
            encoded_inputs[&#34;special_tokens_mask&#34;] = self.get_special_tokens_mask(ids, pair_ids)
        else:
            encoded_inputs[&#34;special_tokens_mask&#34;] = [0] * len(sequence)

    # Padding
    if padding != PaddingStrategy.DO_NOT_PAD or return_attention_mask:
        encoded_inputs = self.pad(
            encoded_inputs,
            max_length=max_length,
            padding=padding,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

    if return_length:
        encoded_inputs[&#34;length&#34;] = len(encoded_inputs[&#34;input_ids&#34;])

    batch_outputs = BatchEncoding(
        encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis
    )
    
    return batch_outputs</code></pre>
</details>
</dd>
<dt id="text_embeddings.base.EmbeddingTokenizer.text2embeddings"><code class="name flex">
<span>def <span class="ident">text2embeddings</span></span>(<span>self, text: str) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def text2embeddings(self, text: str) -&gt; np.ndarray:
    raise NotImplementedError(&#39;This function is not implemented&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="text_embeddings" href="../index.html">text_embeddings</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="text_embeddings.base.is_torch" href="#text_embeddings.base.is_torch">is_torch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="text_embeddings.base.EmbeddingTokenizer" href="#text_embeddings.base.EmbeddingTokenizer">EmbeddingTokenizer</a></code></h4>
<ul class="">
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.build_inputs_with_special_tokens" href="#text_embeddings.base.EmbeddingTokenizer.build_inputs_with_special_tokens">build_inputs_with_special_tokens</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.create_padding_token_embedding" href="#text_embeddings.base.EmbeddingTokenizer.create_padding_token_embedding">create_padding_token_embedding</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.create_token_type_ids_from_sequences" href="#text_embeddings.base.EmbeddingTokenizer.create_token_type_ids_from_sequences">create_token_type_ids_from_sequences</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.get_special_tokens_mask" href="#text_embeddings.base.EmbeddingTokenizer.get_special_tokens_mask">get_special_tokens_mask</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.max_model_input_sizes" href="#text_embeddings.base.EmbeddingTokenizer.max_model_input_sizes">max_model_input_sizes</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.model_input_names" href="#text_embeddings.base.EmbeddingTokenizer.model_input_names">model_input_names</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.num_special_tokens_to_add" href="#text_embeddings.base.EmbeddingTokenizer.num_special_tokens_to_add">num_special_tokens_to_add</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.pad" href="#text_embeddings.base.EmbeddingTokenizer.pad">pad</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.padding_side" href="#text_embeddings.base.EmbeddingTokenizer.padding_side">padding_side</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.prepare_for_model" href="#text_embeddings.base.EmbeddingTokenizer.prepare_for_model">prepare_for_model</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.pretrained_init_configuration" href="#text_embeddings.base.EmbeddingTokenizer.pretrained_init_configuration">pretrained_init_configuration</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.pretrained_vocab_files_map" href="#text_embeddings.base.EmbeddingTokenizer.pretrained_vocab_files_map">pretrained_vocab_files_map</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.text2embeddings" href="#text_embeddings.base.EmbeddingTokenizer.text2embeddings">text2embeddings</a></code></li>
<li><code><a title="text_embeddings.base.EmbeddingTokenizer.vocab_files_names" href="#text_embeddings.base.EmbeddingTokenizer.vocab_files_names">vocab_files_names</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>