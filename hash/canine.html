<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>text_embeddings.hash.canine API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>text_embeddings.hash.canine</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Date    : 2021-04-18 09:06:29
# @Author  : Chenghao Mou (mouchenghao@gmail.com)

import numpy as np
from dataclasses import dataclass
from typing import Optional, List, Union
from transformers.tokenization_utils_base import *
from itertools import zip_longest
from text_embeddings.hash.util import murmurhash

def _is_torch(x):
    import torch

    return isinstance(x, torch.Tensor)


class CANINETokenizer(PreTrainedTokenizerBase):
    
    def __init__(
        self,
        hash_size: int = 768,
        model_input_names: Optional[List[str]] = None,
        special_tokens: Optional[Dict[str, np.ndarray]] = None,
        max_length: Optional[int] = 2048,
    ):
        &#34;&#34;&#34;A character hashing tokenizer/embedder from [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)

        Parameters
        ----------
        hash_size : int, optional
            The embedding size of each character, by default 768
        model_input_names : Optional[List[str]], optional
            Required input names for the downstream model, by default None
        special_tokens : Optional[Dict[str, np.ndarray]], optional
            Special tokens of the downstream model, by default None
        max_length : Optional[int], optional
            Maximum character length, by default 2048

        Examples
        --------
        &gt;&gt;&gt; from text_embeddings.hash import CANINETokenizer
        &gt;&gt;&gt; from transformers.tokenization_utils_base import *
        &gt;&gt;&gt; tokenier = CANINETokenizer()
        &gt;&gt;&gt; results = tokenier(text=[&#39;This is a sentence.&#39;, &#39;This is another sentence.&#39;], padding=PaddingStrategy.LONGEST, truncation=TruncationStrategy.LONGEST_FIRST, add_special_tokens=False)
        &gt;&gt;&gt; assert results[&#39;input_ids&#39;].shape == (2, 25, 768), results[&#39;input_ids&#39;].shape
        &#34;&#34;&#34;

        self.hash_size = hash_size
        self.model_input_names = model_input_names
        self.special_tokens = special_tokens
        self.max_length = max_length

        if self.model_input_names is None:
            # Assume the model takes BERT-like parameters
            self.model_input_names = [&#34;input_ids&#34;, &#34;token_type_ids&#34;, &#34;attention_mask&#34;]

    def __call__(
        self,
        text: Union[TextInput, List[TextInput]],
        text_pair: Optional[Union[TextInput, List[TextInput]]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        **kwargs,
    ) -&gt; BatchEncoding:
        &#34;&#34;&#34;Tokenize the text into a sequence of image blocks.

        Parameters
        ----------
        text : Union[TextInput, List[TextInput]]
            A single text or a list of text
        text_pair : Optional[Union[TextInput, List[TextInput]]], optional
            A single text or a list of text, by default None
        add_special_tokens : bool, optional
            Whether to add special tokens to the data, by default True
        padding : Union[bool, str, PaddingStrategy], optional
            The padding strategy, by default False
        truncation : Union[bool, str, TruncationStrategy], optional
            The truncation strategy, by default False
        max_length : Optional[int], optional
            Maximum sequence length, overriding the class variable, by default None
        stride : int, optional
            Stride for generating blocks, by default 0
        pad_to_multiple_of : Optional[int], optional
            Padding parameters, by default None
        return_tensors : Optional[Union[str, TensorType]], optional
            Return tensors in `pt`, &#39;tf&#39; or &#39;np&#39;, by default None
        return_token_type_ids : Optional[bool], optional
            Return token type ids, by default None
        return_attention_mask : Optional[bool], optional
            Return attention mask, by default None
        return_overflowing_tokens : bool, optional
            Return overflowing tokens, by default False
        return_special_tokens_mask : bool, optional
            Return special token mask, by default False
        return_length : bool, optional
            Return length, by default False

        Returns
        -------
        BatchEncoding
            A BatchEncoding object
        &#34;&#34;&#34;
        if self.special_tokens is None:
            self.special_tokens = {
                &#34;CLS&#34;: self.text2hashes(&#34;[CLS]&#34;),
                &#34;SEP&#34;: self.text2hashes(&#34;[SEP]&#34;),
            }

        if add_special_tokens and text_pair:
            actual_max_length = self.max_length - len(self.special_tokens[&#34;SEP&#34;]) * 2 - len(self.special_tokens[&#34;CLS&#34;])
        else:
            actual_max_length = self.max_length
        
        batch_outputs = {}
        text = text if isinstance(text, list) else [text]
        text_pair = text_pair if isinstance(text_pair, list) else [text_pair]

        for first_text, second_text in zip_longest(text, text_pair):
            
            first_embeddings = self.text2hashes(first_text)
            second_embeddings = self.text2hashes(second_text)

            outputs = self.prepare_for_model(
                first_embeddings,
                second_embeddings,
                add_special_tokens=add_special_tokens,
                padding=PaddingStrategy.DO_NOT_PAD,  # we pad in batch afterward
                truncation=truncation,
                max_length=max_length or actual_max_length,
                stride=stride,
                pad_to_multiple_of=None,  # we pad in batch afterward
                return_attention_mask=False,  # we pad in batch afterward
                return_token_type_ids=return_token_type_ids,
                return_overflowing_tokens=return_overflowing_tokens,
                return_special_tokens_mask=return_special_tokens_mask,
                return_length=return_length,
                return_tensors=None,  # We convert the whole batch to tensors at the end
                prepend_batch_axis=False,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        
        batch_outputs = self.pad(
            batch_outputs,
            padding=padding,
            max_length=max_length or actual_max_length,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)

        if &#34;input_ids&#34; in batch_outputs:
            batch_outputs[&#34;input_ids&#34;] = np.squeeze(batch_outputs[&#34;input_ids&#34;], axis=2)

        return batch_outputs

    def text2hashes(self, text: str) -&gt; np.ndarray:
        &#34;&#34;&#34;Convert text into an numpy array, in (sequence_length, 1, hash_size) shape.

        Parameters
        ----------
        text : str
            Input text

        Returns
        -------
        np.ndarray
            An array in (sequence_length, 1, hash_size) shape
        &#34;&#34;&#34;
        if not text:
            return None
        
        result = np.zeros((len(text), self.hash_size))
        for i, char in enumerate(text):
            result[i] = murmurhash(char, feature_size=self.hash_size*2)
        
        return np.expand_dims(result, axis=1)

    def prepare_for_model(
        self,
        ids: List[int],
        pair_ids: Optional[List[int]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        prepend_batch_axis: bool = False,
        **kwargs
    ):

        pair = bool(pair_ids is not None)
        len_ids = len(ids)
        len_pair_ids = len(pair_ids) if pair else 0

        if return_token_type_ids and not add_special_tokens:
            raise ValueError(
                &#34;Asking to return token_type_ids while setting add_special_tokens to False &#34;
                &#34;results in an undefined behavior. Please set add_special_tokens to True or &#34;
                &#34;set return_token_type_ids to None.&#34;
            )

        # Load from model defaults
        if return_token_type_ids is None:
            return_token_type_ids = &#34;token_type_ids&#34; in self.model_input_names
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        encoded_inputs = {}

        # Compute the total size of the returned encodings
        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)

        # Truncation: Handle max sequence length
        overflowing_tokens = []
        if truncation != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len &gt; max_length:
            ids, pair_ids, overflowing_tokens = self.truncate_sequences(
                ids,
                pair_ids=pair_ids,
                num_tokens_to_remove=total_len - max_length,
                truncation_strategy=truncation,
                stride=stride,
            )

        if return_overflowing_tokens:
            encoded_inputs[&#34;overflowing_tokens&#34;] = overflowing_tokens
            encoded_inputs[&#34;num_truncated_tokens&#34;] = total_len - max_length

        # Add special tokens
        if add_special_tokens:
            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)
            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)
        else:
            sequence = np.concatenate([ids, pair_ids], axis=0) if pair is True else ids
            token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])
        
        # Build output dictionary
        encoded_inputs[&#34;input_ids&#34;] = sequence

        if return_token_type_ids:
            encoded_inputs[&#34;token_type_ids&#34;] = token_type_ids
        if return_special_tokens_mask:
            if add_special_tokens:
                encoded_inputs[&#34;special_tokens_mask&#34;] = self.get_special_tokens_mask(ids, pair_ids)
            else:
                encoded_inputs[&#34;special_tokens_mask&#34;] = [0] * len(sequence)

        # Padding
        if padding != PaddingStrategy.DO_NOT_PAD or return_attention_mask:
            encoded_inputs = self.pad(
                encoded_inputs,
                max_length=max_length,
                padding=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

        if return_length:
            encoded_inputs[&#34;length&#34;] = len(encoded_inputs[&#34;input_ids&#34;])

        
        batch_outputs = BatchEncoding(
            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis
        )
        
        return batch_outputs
    
    def num_special_tokens_to_add(self, pair: bool = False) -&gt; int:
        return 0 if not pair else 3
    
    def build_inputs_with_special_tokens(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return token_ids_0
        
        return np.concatenate(
            [
                self.special_tokens[&#34;CLS&#34;],
                token_ids_0,
                self.special_tokens[&#34;SEP&#34;],
                token_ids_1,
                self.special_tokens[&#34;SEP&#34;],
            ],
            axis=0
        )
    
    def get_special_tokens_mask(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return [0 for _ in token_ids_0]
        return [1 for _ in self.special_tokens[&#34;CLS&#34;]] + [0 for _ in token_ids_0] + [1 for _ in self.special_tokens[&#34;SEP&#34;]] + [0 for _ in token_ids_1] + [1 for _ in self.special_tokens[&#34;SEP&#34;]]
    
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:

        if token_ids_1 is None:
            return len(token_ids_0) * [0]
        return [0]*len(self.special_tokens[&#34;CLS&#34;]) + [0] * len(token_ids_0) + [0]*len(self.special_tokens[&#34;SEP&#34;]) + [1] * len(token_ids_1) + [0]*len(self.special_tokens[&#34;SEP&#34;])

    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            List[BatchEncoding],
            Dict[str, EncodedInput],
            Dict[str, List[EncodedInput]],
            List[Dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = True,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        verbose: bool = True,
    ) -&gt; BatchEncoding:

        # If we have a list of dicts, let&#39;s convert it in a dict of lists
        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):
            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}

        # The model&#39;s main input name, usually `input_ids`, has be passed for padding
        if self.model_input_names[0] not in encoded_inputs:
            raise ValueError(
                &#34;You should supply an encoding or a list of encodings to this method&#34;
                f&#34;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&#34;
            )

        required_input = encoded_inputs[self.model_input_names[0]]

        if required_input is None:
            if return_attention_mask:
                encoded_inputs[&#34;attention_mask&#34;] = []
            return encoded_inputs

        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects
        # and rebuild them afterwards if no return_tensors is specified
        # Note that we lose the specific device the tensor may be on for PyTorch

        first_element = required_input[0]
        if isinstance(first_element, (list, tuple)):
            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
            index = 0
            while len(required_input[index]) == 0:
                index += 1
            if index &lt; len(required_input):
                first_element = required_input[index][0]
        # At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.
        if not isinstance(first_element, (int, list, tuple)):
            if is_torch_available() and _is_torch(first_element):
                return_tensors = &#34;pt&#34; if return_tensors is None else return_tensors
            elif isinstance(first_element, np.ndarray):
                return_tensors = &#34;np&#34; if return_tensors is None else return_tensors
            else:
                raise ValueError(
                    f&#34;type of {first_element} unknown: {type(first_element)}. &#34;
                    f&#34;Should be one of a python, numpy or pytorch object.&#34;
                )

            for key, value in encoded_inputs.items():
                encoded_inputs[key] = to_py_obj(value)
        
        required_input = encoded_inputs[self.model_input_names[0]]
        if required_input and not isinstance(required_input[0], (list, tuple)):
            encoded_inputs = self._pad(
                encoded_inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )
            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)

        batch_size = len(required_input)
        assert all(
            len(v) == batch_size for v in encoded_inputs.values()
        ), &#34;Some items in the output dictionary have a different batch size than others.&#34;

        if padding == PaddingStrategy.LONGEST:
            max_length = max(len(inputs) for inputs in required_input)
            padding = PaddingStrategy.MAX_LENGTH

        batch_outputs = {}
        for i in range(batch_size):
            inputs = dict((k, v[i]) for k, v in encoded_inputs.items())
            outputs = self._pad(
                inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        return BatchEncoding(batch_outputs, tensor_type=return_tensors)

    def _pad(
        self,
        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
        max_length: Optional[int] = None,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
    ) -&gt; dict:

        # Load from model defaults
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        required_input = encoded_inputs[self.model_input_names[0]]

        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length

        if needs_to_be_padded:
            difference = max_length - len(required_input)
            if &#34;token_type_ids&#34; in encoded_inputs and isinstance(encoded_inputs[&#34;token_type_ids&#34;], int):
                encoded_inputs[&#34;token_type_ids&#34;] = [encoded_inputs[&#34;token_type_ids&#34;]]
            if self.padding_side == &#34;right&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input) + [0] * difference
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = (
                        encoded_inputs[&#34;token_type_ids&#34;] + [1] * difference
                    )
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = encoded_inputs[&#34;special_tokens_mask&#34;] + [1] * difference
                
                encoded_inputs[self.model_input_names[0]] = required_input + [np.zeros((len(required_input[0]), self.hash_size))] * difference
            elif self.padding_side == &#34;left&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [0] * difference + [1] * len(required_input)
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = [1] * difference + encoded_inputs[
                        &#34;token_type_ids&#34;
                    ]
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = [1] * difference + encoded_inputs[&#34;special_tokens_mask&#34;]
                encoded_inputs[self.model_input_names[0]] = [np.zeros((len(required_input[0]), self.hash_size))] * difference + required_input
            else:
                raise ValueError(&#34;Invalid padding strategy:&#34; + str(self.padding_side))
        elif return_attention_mask and &#34;attention_mask&#34; not in encoded_inputs:
            encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input)

        return encoded_inputs</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="text_embeddings.hash.canine.CANINETokenizer"><code class="flex name class">
<span>class <span class="ident">CANINETokenizer</span></span>
<span>(</span><span>hash_size: int = 768, model_input_names: Union[List[str], NoneType] = None, special_tokens: Union[Dict[str, numpy.ndarray], NoneType] = None, max_length: Union[int, NoneType] = 2048)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for :class:<code>~transformers.PreTrainedTokenizer</code> and :class:<code>~transformers.PreTrainedTokenizerFast</code>.</p>
<p>Handles shared (mostly boiler plate) methods for those two classes.</p>
<p>Class attributes (overridden by derived classes)</p>
<pre><code>- **vocab_files_names** (:obj:&lt;code&gt;Dict\[str, str]&lt;/code&gt;) -- A dictionary with, as keys, the &lt;code&gt;\_\_init\_\_&lt;/code&gt; keyword name of
  each vocabulary file required by the model, and as associated values, the filename for saving the associated
  file (string).
- **pretrained_vocab_files_map** (:obj:&lt;code&gt;Dict\[str, Dict\[str, str]]&lt;/code&gt;) -- A dictionary of dictionaries, with the
  high-level keys being the &lt;code&gt;\_\_init\_\_&lt;/code&gt; keyword name of each vocabulary file required by the model, the
  low-level being the :obj:`short-cut-names` of the pretrained models with, as associated values, the
  :obj:&lt;code&gt;url&lt;/code&gt; to the associated pretrained vocabulary file.
- **max_model_input_sizes** (:obj:&lt;code&gt;Dict\[str, Optinal\[int]]&lt;/code&gt;) -- A dictionary with, as keys, the
  :obj:`short-cut-names` of the pretrained models, and as associated values, the maximum length of the sequence
  inputs of this model, or :obj:&lt;code&gt;None&lt;/code&gt; if the model has no maximum input size.
- **pretrained_init_configuration** (:obj:&lt;code&gt;Dict\[str, Dict\[str, Any]]&lt;/code&gt;) -- A dictionary with, as keys, the
  :obj:`short-cut-names` of the pretrained models, and as associated values, a dictionary of specific arguments
  to pass to the &lt;code&gt;\_\_init\_\_&lt;/code&gt; method of the tokenizer class for this pretrained model when loading the
  tokenizer with the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`
  method.
- **model_input_names** (:obj:&lt;code&gt;List\[str]&lt;/code&gt;) -- A list of inputs expected in the forward pass of the model.
- **padding_side** (:obj:&lt;code&gt;str&lt;/code&gt;) -- The default value for the side on which the model should have padding
  applied. Should be :obj:`'right'` or :obj:`'left'`.
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt>model_max_length (:obj:<code>int</code>, <code>optional</code>):</dt>
<dt>The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is</dt>
<dt>loaded with :meth:<code>~transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained</code>, this</dt>
<dt>will be set to the value stored for the associated model in <code>max_model_input_sizes</code> (see above). If no</dt>
<dt>value is provided, will default to VERY_LARGE_INTEGER (:obj:<code>int(1e30)</code>).</dt>
<dt><strong><code>padding_side</code></strong></dt>
<dd>(:obj:<code>str</code>, <code>optional</code>):
The side on which the model should have padding applied. Should be selected between ['right', 'left'].
Default value is picked from the class attribute of the same name.</dd>
</dl>
<p>model_input_names (:obj:<code>List[string]</code>, <code>optional</code>):
The list of inputs accepted by the forward pass of the model (like :obj:<code>"token_type_ids"</code> or
:obj:<code>"attention_mask"</code>). Default value is picked from the class attribute of the same name.
bos_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token representing the beginning of a sentence. Will be associated to <code>self.bos_token</code> and
<code>self.bos_token_id</code>.
eos_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token representing the end of a sentence. Will be associated to <code>self.eos_token</code> and
<code>self.eos_token_id</code>.
unk_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token representing an out-of-vocabulary token. Will be associated to <code>self.unk_token</code> and
<code>self.unk_token_id</code>.
sep_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code>self.sep_token</code> and <code>self.sep_token_id</code>.
pad_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code>self.pad_token</code> and
<code>self.pad_token_id</code>.
cls_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token representing the class of the input (used by BERT for instance). Will be associated to
<code>self.cls_token</code> and <code>self.cls_token_id</code>.
mask_token (:obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code>self.mask_token</code> and <code>self.mask_token_id</code>.
additional_special_tokens (tuple or list of :obj:<code>str</code> or :obj:<code>tokenizers.AddedToken</code>, <code>optional</code>):
A tuple or a list of additional special tokens. Add them here to ensure they won't be split by the
tokenization process. Will be associated to <code>self.additional_special_tokens</code> and
<code>self.additional_special_tokens_ids</code>.
A character hashing tokenizer/embedder from <a href="https://arxiv.org/abs/2103.06874">CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hash_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The embedding size of each character, by default 768</dd>
<dt><strong><code>model_input_names</code></strong> :&ensp;<code>Optional[List[str]]</code>, optional</dt>
<dd>Required input names for the downstream model, by default None</dd>
<dt><strong><code>special_tokens</code></strong> :&ensp;<code>Optional[Dict[str, np.ndarray]]</code>, optional</dt>
<dd>Special tokens of the downstream model, by default None</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>Optional[int]</code>, optional</dt>
<dd>Maximum character length, by default 2048</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from text_embeddings.hash import CANINETokenizer
&gt;&gt;&gt; from transformers.tokenization_utils_base import *
&gt;&gt;&gt; tokenier = CANINETokenizer()
&gt;&gt;&gt; results = tokenier(text=['This is a sentence.', 'This is another sentence.'], padding=PaddingStrategy.LONGEST, truncation=TruncationStrategy.LONGEST_FIRST, add_special_tokens=False)
&gt;&gt;&gt; assert results['input_ids'].shape == (2, 25, 768), results['input_ids'].shape
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CANINETokenizer(PreTrainedTokenizerBase):
    
    def __init__(
        self,
        hash_size: int = 768,
        model_input_names: Optional[List[str]] = None,
        special_tokens: Optional[Dict[str, np.ndarray]] = None,
        max_length: Optional[int] = 2048,
    ):
        &#34;&#34;&#34;A character hashing tokenizer/embedder from [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)

        Parameters
        ----------
        hash_size : int, optional
            The embedding size of each character, by default 768
        model_input_names : Optional[List[str]], optional
            Required input names for the downstream model, by default None
        special_tokens : Optional[Dict[str, np.ndarray]], optional
            Special tokens of the downstream model, by default None
        max_length : Optional[int], optional
            Maximum character length, by default 2048

        Examples
        --------
        &gt;&gt;&gt; from text_embeddings.hash import CANINETokenizer
        &gt;&gt;&gt; from transformers.tokenization_utils_base import *
        &gt;&gt;&gt; tokenier = CANINETokenizer()
        &gt;&gt;&gt; results = tokenier(text=[&#39;This is a sentence.&#39;, &#39;This is another sentence.&#39;], padding=PaddingStrategy.LONGEST, truncation=TruncationStrategy.LONGEST_FIRST, add_special_tokens=False)
        &gt;&gt;&gt; assert results[&#39;input_ids&#39;].shape == (2, 25, 768), results[&#39;input_ids&#39;].shape
        &#34;&#34;&#34;

        self.hash_size = hash_size
        self.model_input_names = model_input_names
        self.special_tokens = special_tokens
        self.max_length = max_length

        if self.model_input_names is None:
            # Assume the model takes BERT-like parameters
            self.model_input_names = [&#34;input_ids&#34;, &#34;token_type_ids&#34;, &#34;attention_mask&#34;]

    def __call__(
        self,
        text: Union[TextInput, List[TextInput]],
        text_pair: Optional[Union[TextInput, List[TextInput]]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        **kwargs,
    ) -&gt; BatchEncoding:
        &#34;&#34;&#34;Tokenize the text into a sequence of image blocks.

        Parameters
        ----------
        text : Union[TextInput, List[TextInput]]
            A single text or a list of text
        text_pair : Optional[Union[TextInput, List[TextInput]]], optional
            A single text or a list of text, by default None
        add_special_tokens : bool, optional
            Whether to add special tokens to the data, by default True
        padding : Union[bool, str, PaddingStrategy], optional
            The padding strategy, by default False
        truncation : Union[bool, str, TruncationStrategy], optional
            The truncation strategy, by default False
        max_length : Optional[int], optional
            Maximum sequence length, overriding the class variable, by default None
        stride : int, optional
            Stride for generating blocks, by default 0
        pad_to_multiple_of : Optional[int], optional
            Padding parameters, by default None
        return_tensors : Optional[Union[str, TensorType]], optional
            Return tensors in `pt`, &#39;tf&#39; or &#39;np&#39;, by default None
        return_token_type_ids : Optional[bool], optional
            Return token type ids, by default None
        return_attention_mask : Optional[bool], optional
            Return attention mask, by default None
        return_overflowing_tokens : bool, optional
            Return overflowing tokens, by default False
        return_special_tokens_mask : bool, optional
            Return special token mask, by default False
        return_length : bool, optional
            Return length, by default False

        Returns
        -------
        BatchEncoding
            A BatchEncoding object
        &#34;&#34;&#34;
        if self.special_tokens is None:
            self.special_tokens = {
                &#34;CLS&#34;: self.text2hashes(&#34;[CLS]&#34;),
                &#34;SEP&#34;: self.text2hashes(&#34;[SEP]&#34;),
            }

        if add_special_tokens and text_pair:
            actual_max_length = self.max_length - len(self.special_tokens[&#34;SEP&#34;]) * 2 - len(self.special_tokens[&#34;CLS&#34;])
        else:
            actual_max_length = self.max_length
        
        batch_outputs = {}
        text = text if isinstance(text, list) else [text]
        text_pair = text_pair if isinstance(text_pair, list) else [text_pair]

        for first_text, second_text in zip_longest(text, text_pair):
            
            first_embeddings = self.text2hashes(first_text)
            second_embeddings = self.text2hashes(second_text)

            outputs = self.prepare_for_model(
                first_embeddings,
                second_embeddings,
                add_special_tokens=add_special_tokens,
                padding=PaddingStrategy.DO_NOT_PAD,  # we pad in batch afterward
                truncation=truncation,
                max_length=max_length or actual_max_length,
                stride=stride,
                pad_to_multiple_of=None,  # we pad in batch afterward
                return_attention_mask=False,  # we pad in batch afterward
                return_token_type_ids=return_token_type_ids,
                return_overflowing_tokens=return_overflowing_tokens,
                return_special_tokens_mask=return_special_tokens_mask,
                return_length=return_length,
                return_tensors=None,  # We convert the whole batch to tensors at the end
                prepend_batch_axis=False,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        
        batch_outputs = self.pad(
            batch_outputs,
            padding=padding,
            max_length=max_length or actual_max_length,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)

        if &#34;input_ids&#34; in batch_outputs:
            batch_outputs[&#34;input_ids&#34;] = np.squeeze(batch_outputs[&#34;input_ids&#34;], axis=2)

        return batch_outputs

    def text2hashes(self, text: str) -&gt; np.ndarray:
        &#34;&#34;&#34;Convert text into an numpy array, in (sequence_length, 1, hash_size) shape.

        Parameters
        ----------
        text : str
            Input text

        Returns
        -------
        np.ndarray
            An array in (sequence_length, 1, hash_size) shape
        &#34;&#34;&#34;
        if not text:
            return None
        
        result = np.zeros((len(text), self.hash_size))
        for i, char in enumerate(text):
            result[i] = murmurhash(char, feature_size=self.hash_size*2)
        
        return np.expand_dims(result, axis=1)

    def prepare_for_model(
        self,
        ids: List[int],
        pair_ids: Optional[List[int]] = None,
        add_special_tokens: bool = True,
        padding: Union[bool, str, PaddingStrategy] = False,
        truncation: Union[bool, str, TruncationStrategy] = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_length: bool = False,
        prepend_batch_axis: bool = False,
        **kwargs
    ):

        pair = bool(pair_ids is not None)
        len_ids = len(ids)
        len_pair_ids = len(pair_ids) if pair else 0

        if return_token_type_ids and not add_special_tokens:
            raise ValueError(
                &#34;Asking to return token_type_ids while setting add_special_tokens to False &#34;
                &#34;results in an undefined behavior. Please set add_special_tokens to True or &#34;
                &#34;set return_token_type_ids to None.&#34;
            )

        # Load from model defaults
        if return_token_type_ids is None:
            return_token_type_ids = &#34;token_type_ids&#34; in self.model_input_names
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        encoded_inputs = {}

        # Compute the total size of the returned encodings
        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)

        # Truncation: Handle max sequence length
        overflowing_tokens = []
        if truncation != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len &gt; max_length:
            ids, pair_ids, overflowing_tokens = self.truncate_sequences(
                ids,
                pair_ids=pair_ids,
                num_tokens_to_remove=total_len - max_length,
                truncation_strategy=truncation,
                stride=stride,
            )

        if return_overflowing_tokens:
            encoded_inputs[&#34;overflowing_tokens&#34;] = overflowing_tokens
            encoded_inputs[&#34;num_truncated_tokens&#34;] = total_len - max_length

        # Add special tokens
        if add_special_tokens:
            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)
            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)
        else:
            sequence = np.concatenate([ids, pair_ids], axis=0) if pair is True else ids
            token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])
        
        # Build output dictionary
        encoded_inputs[&#34;input_ids&#34;] = sequence

        if return_token_type_ids:
            encoded_inputs[&#34;token_type_ids&#34;] = token_type_ids
        if return_special_tokens_mask:
            if add_special_tokens:
                encoded_inputs[&#34;special_tokens_mask&#34;] = self.get_special_tokens_mask(ids, pair_ids)
            else:
                encoded_inputs[&#34;special_tokens_mask&#34;] = [0] * len(sequence)

        # Padding
        if padding != PaddingStrategy.DO_NOT_PAD or return_attention_mask:
            encoded_inputs = self.pad(
                encoded_inputs,
                max_length=max_length,
                padding=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

        if return_length:
            encoded_inputs[&#34;length&#34;] = len(encoded_inputs[&#34;input_ids&#34;])

        
        batch_outputs = BatchEncoding(
            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis
        )
        
        return batch_outputs
    
    def num_special_tokens_to_add(self, pair: bool = False) -&gt; int:
        return 0 if not pair else 3
    
    def build_inputs_with_special_tokens(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return token_ids_0
        
        return np.concatenate(
            [
                self.special_tokens[&#34;CLS&#34;],
                token_ids_0,
                self.special_tokens[&#34;SEP&#34;],
                token_ids_1,
                self.special_tokens[&#34;SEP&#34;],
            ],
            axis=0
        )
    
    def get_special_tokens_mask(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:
        if token_ids_1 is None:
            return [0 for _ in token_ids_0]
        return [1 for _ in self.special_tokens[&#34;CLS&#34;]] + [0 for _ in token_ids_0] + [1 for _ in self.special_tokens[&#34;SEP&#34;]] + [0 for _ in token_ids_1] + [1 for _ in self.special_tokens[&#34;SEP&#34;]]
    
    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -&gt; List[int]:

        if token_ids_1 is None:
            return len(token_ids_0) * [0]
        return [0]*len(self.special_tokens[&#34;CLS&#34;]) + [0] * len(token_ids_0) + [0]*len(self.special_tokens[&#34;SEP&#34;]) + [1] * len(token_ids_1) + [0]*len(self.special_tokens[&#34;SEP&#34;])

    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            List[BatchEncoding],
            Dict[str, EncodedInput],
            Dict[str, List[EncodedInput]],
            List[Dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = True,
        max_length: Optional[int] = None,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
        return_tensors: Optional[Union[str, TensorType]] = None,
        verbose: bool = True,
    ) -&gt; BatchEncoding:

        # If we have a list of dicts, let&#39;s convert it in a dict of lists
        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):
            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}

        # The model&#39;s main input name, usually `input_ids`, has be passed for padding
        if self.model_input_names[0] not in encoded_inputs:
            raise ValueError(
                &#34;You should supply an encoding or a list of encodings to this method&#34;
                f&#34;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&#34;
            )

        required_input = encoded_inputs[self.model_input_names[0]]

        if required_input is None:
            if return_attention_mask:
                encoded_inputs[&#34;attention_mask&#34;] = []
            return encoded_inputs

        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects
        # and rebuild them afterwards if no return_tensors is specified
        # Note that we lose the specific device the tensor may be on for PyTorch

        first_element = required_input[0]
        if isinstance(first_element, (list, tuple)):
            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
            index = 0
            while len(required_input[index]) == 0:
                index += 1
            if index &lt; len(required_input):
                first_element = required_input[index][0]
        # At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.
        if not isinstance(first_element, (int, list, tuple)):
            if is_torch_available() and _is_torch(first_element):
                return_tensors = &#34;pt&#34; if return_tensors is None else return_tensors
            elif isinstance(first_element, np.ndarray):
                return_tensors = &#34;np&#34; if return_tensors is None else return_tensors
            else:
                raise ValueError(
                    f&#34;type of {first_element} unknown: {type(first_element)}. &#34;
                    f&#34;Should be one of a python, numpy or pytorch object.&#34;
                )

            for key, value in encoded_inputs.items():
                encoded_inputs[key] = to_py_obj(value)
        
        required_input = encoded_inputs[self.model_input_names[0]]
        if required_input and not isinstance(required_input[0], (list, tuple)):
            encoded_inputs = self._pad(
                encoded_inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )
            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)

        batch_size = len(required_input)
        assert all(
            len(v) == batch_size for v in encoded_inputs.values()
        ), &#34;Some items in the output dictionary have a different batch size than others.&#34;

        if padding == PaddingStrategy.LONGEST:
            max_length = max(len(inputs) for inputs in required_input)
            padding = PaddingStrategy.MAX_LENGTH

        batch_outputs = {}
        for i in range(batch_size):
            inputs = dict((k, v[i]) for k, v in encoded_inputs.items())
            outputs = self._pad(
                inputs,
                max_length=max_length,
                padding_strategy=padding,
                pad_to_multiple_of=pad_to_multiple_of,
                return_attention_mask=return_attention_mask,
            )

            for key, value in outputs.items():
                if key not in batch_outputs:
                    batch_outputs[key] = []
                batch_outputs[key].append(value)

        return BatchEncoding(batch_outputs, tensor_type=return_tensors)

    def _pad(
        self,
        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
        max_length: Optional[int] = None,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        pad_to_multiple_of: Optional[int] = None,
        return_attention_mask: Optional[bool] = None,
    ) -&gt; dict:

        # Load from model defaults
        if return_attention_mask is None:
            return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

        required_input = encoded_inputs[self.model_input_names[0]]

        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length

        if needs_to_be_padded:
            difference = max_length - len(required_input)
            if &#34;token_type_ids&#34; in encoded_inputs and isinstance(encoded_inputs[&#34;token_type_ids&#34;], int):
                encoded_inputs[&#34;token_type_ids&#34;] = [encoded_inputs[&#34;token_type_ids&#34;]]
            if self.padding_side == &#34;right&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input) + [0] * difference
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = (
                        encoded_inputs[&#34;token_type_ids&#34;] + [1] * difference
                    )
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = encoded_inputs[&#34;special_tokens_mask&#34;] + [1] * difference
                
                encoded_inputs[self.model_input_names[0]] = required_input + [np.zeros((len(required_input[0]), self.hash_size))] * difference
            elif self.padding_side == &#34;left&#34;:
                if return_attention_mask:
                    encoded_inputs[&#34;attention_mask&#34;] = [0] * difference + [1] * len(required_input)
                if &#34;token_type_ids&#34; in encoded_inputs:
                    encoded_inputs[&#34;token_type_ids&#34;] = [1] * difference + encoded_inputs[
                        &#34;token_type_ids&#34;
                    ]
                if &#34;special_tokens_mask&#34; in encoded_inputs:
                    encoded_inputs[&#34;special_tokens_mask&#34;] = [1] * difference + encoded_inputs[&#34;special_tokens_mask&#34;]
                encoded_inputs[self.model_input_names[0]] = [np.zeros((len(required_input[0]), self.hash_size))] * difference + required_input
            else:
                raise ValueError(&#34;Invalid padding strategy:&#34; + str(self.padding_side))
        elif return_attention_mask and &#34;attention_mask&#34; not in encoded_inputs:
            encoded_inputs[&#34;attention_mask&#34;] = [1] * len(required_input)

        return encoded_inputs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.tokenization_utils_base.PreTrainedTokenizerBase</li>
<li>transformers.tokenization_utils_base.SpecialTokensMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="text_embeddings.hash.canine.CANINETokenizer.max_model_input_sizes"><code class="name">var <span class="ident">max_model_input_sizes</span> : Dict[str, Union[int, NoneType]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.model_input_names"><code class="name">var <span class="ident">model_input_names</span> : List[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.padding_side"><code class="name">var <span class="ident">padding_side</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.pretrained_init_configuration"><code class="name">var <span class="ident">pretrained_init_configuration</span> : Dict[str, Dict[str, Any]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.pretrained_vocab_files_map"><code class="name">var <span class="ident">pretrained_vocab_files_map</span> : Dict[str, Dict[str, str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.vocab_files_names"><code class="name">var <span class="ident">vocab_files_names</span> : Dict[str, str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="text_embeddings.hash.canine.CANINETokenizer.build_inputs_with_special_tokens"><code class="name flex">
<span>def <span class="ident">build_inputs_with_special_tokens</span></span>(<span>self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.</p>
<p>This implementation does not add special tokens and this method should be overridden in a subclass.</p>
<h2 id="args">Args</h2>
<p>token_ids_0 (:obj:<code>List[int]</code>): The first tokenized sequence.
token_ids_1 (:obj:<code>List[int]</code>, <code>optional</code>): The second tokenized sequence.</p>
<h2 id="returns">Returns</h2>
<p>:obj:<code>List[int]</code>: The model input with special tokens.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_inputs_with_special_tokens(
    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -&gt; List[int]:
    if token_ids_1 is None:
        return token_ids_0
    
    return np.concatenate(
        [
            self.special_tokens[&#34;CLS&#34;],
            token_ids_0,
            self.special_tokens[&#34;SEP&#34;],
            token_ids_1,
            self.special_tokens[&#34;SEP&#34;],
        ],
        axis=0
    )</code></pre>
</details>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.create_token_type_ids_from_sequences"><code class="name flex">
<span>def <span class="ident">create_token_type_ids_from_sequences</span></span>(<span>self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Create the token type IDs corresponding to the sequences passed. <code>What are token type IDs?
&lt;../glossary.html#token-type-ids&gt;</code>__</p>
<p>Should be overridden in a subclass if the model has a special way of building those.</p>
<h2 id="args">Args</h2>
<p>token_ids_0 (:obj:<code>List[int]</code>): The first tokenized sequence.
token_ids_1 (:obj:<code>List[int]</code>, <code>optional</code>): The second tokenized sequence.</p>
<h2 id="returns">Returns</h2>
<p>:obj:<code>List[int]</code>: The token type ids.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_token_type_ids_from_sequences(
    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -&gt; List[int]:

    if token_ids_1 is None:
        return len(token_ids_0) * [0]
    return [0]*len(self.special_tokens[&#34;CLS&#34;]) + [0] * len(token_ids_0) + [0]*len(self.special_tokens[&#34;SEP&#34;]) + [1] * len(token_ids_1) + [0]*len(self.special_tokens[&#34;SEP&#34;])</code></pre>
</details>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.get_special_tokens_mask"><code class="name flex">
<span>def <span class="ident">get_special_tokens_mask</span></span>(<span>self, token_ids_0: List[int], token_ids_1: Union[List[int], NoneType] = None) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> or <code>encode_plus</code> methods.</p>
<h2 id="args">Args</h2>
<p>token_ids_0 (:obj:<code>List[int]</code>):
List of ids of the first sequence.
token_ids_1 (:obj:<code>List[int]</code>, <code>optional</code>):
List of ids of the second sequence.
already_has_special_tokens (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not the token list is already formatted with special tokens for the model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>A list</code> of <code>integers in the range [0, 1]</code></dt>
<dd>1 for a special token, 0 for a sequence token.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_special_tokens_mask(
    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
) -&gt; List[int]:
    if token_ids_1 is None:
        return [0 for _ in token_ids_0]
    return [1 for _ in self.special_tokens[&#34;CLS&#34;]] + [0 for _ in token_ids_0] + [1 for _ in self.special_tokens[&#34;SEP&#34;]] + [0 for _ in token_ids_1] + [1 for _ in self.special_tokens[&#34;SEP&#34;]]</code></pre>
</details>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.num_special_tokens_to_add"><code class="name flex">
<span>def <span class="ident">num_special_tokens_to_add</span></span>(<span>self, pair: bool = False) ‑> int</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def num_special_tokens_to_add(self, pair: bool = False) -&gt; int:
    return 0 if not pair else 3</code></pre>
</details>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: Union[int, NoneType] = None, pad_to_multiple_of: Union[int, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, verbose: bool = True) ‑> transformers.tokenization_utils_base.BatchEncoding</span>
</code></dt>
<dd>
<div class="desc"><p>Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
in the batch.</p>
<p>Padding side (left/right) padding token ids are defined at the tokenizer level (with <code>self.padding_side</code>,
<code>self.pad_token_id</code> and <code>self.pad_token_type_id</code>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code>encoded_inputs</code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code>return_tensors</code>. In the
case of PyTorch tensors, you will lose the specific device of your tensors however.</p>
</div>
<h2 id="args">Args</h2>
<p>encoded_inputs (:class:<code>~transformers.BatchEncoding</code>, list of :class:<code>~transformers.BatchEncoding</code>, :obj:<code>Dict[str, List[int]]</code>, :obj:<code>Dict[str, List[List[int]]</code> or :obj:<code>List[Dict[str, List[int]]]</code>):
Tokenized inputs. Can represent one input (:class:<code>~transformers.BatchEncoding</code> or :obj:<code>Dict[str,
List[int]]</code>) or a batch of tokenized inputs (list of :class:<code>~transformers.BatchEncoding&lt;code&gt;, &lt;/code&gt;Dict[str,
List[List[int]]]&lt;code&gt; or &lt;/code&gt;List[Dict[str, List[int]]]</code>) so you can use this method during preprocessing as
well as in a PyTorch Dataloader collate function.</p>
<pre><code>Instead of :obj:&lt;code&gt;List\[int]&lt;/code&gt; you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.
</code></pre>
<p>padding (:obj:<code>bool</code>, :obj:<code>str</code> or :class:<code>~transformers.file_utils.PaddingStrategy</code>, <code>optional</code>, defaults to :obj:<code>True</code>):
Select a strategy to pad the returned sequences (according to the model's padding side and padding
index) among:</p>
<pre><code>* :obj:&lt;code&gt;True&lt;/code&gt; or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a
  single sequence if provided).
* :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or to the
  maximum acceptable input length for the model if that argument is not provided.
* :obj:&lt;code&gt;False&lt;/code&gt; or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
  different lengths).
</code></pre>
<p>max_length (:obj:<code>int</code>, <code>optional</code>):
Maximum length of the returned list and optionally padding length (see above).
pad_to_multiple_of (:obj:<code>int</code>, <code>optional</code>):
If set will pad the sequence to a multiple of the provided value.</p>
<pre><code>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
&gt;= 7.5 (Volta).
</code></pre>
<p>return_attention_mask (:obj:<code>bool</code>, <code>optional</code>):
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer's default, defined by the :obj:<code>return_outputs</code> attribute.</p>
<pre><code>`What are attention masks? &lt;../glossary.html#attention-mask&gt;`__
</code></pre>
<p>return_tensors (:obj:<code>str</code> or :class:<code>~transformers.file_utils.TensorType</code>, <code>optional</code>):
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<pre><code>* :obj:`'tf'`: Return TensorFlow :obj:&lt;code&gt;tf.constant&lt;/code&gt; objects.
* :obj:`'pt'`: Return PyTorch :obj:&lt;code&gt;torch.Tensor&lt;/code&gt; objects.
* :obj:`'np'`: Return Numpy :obj:&lt;code&gt;np.ndarray&lt;/code&gt; objects.
</code></pre>
<p>verbose (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>True</code>):
Whether or not to print more information and warnings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(
    self,
    encoded_inputs: Union[
        BatchEncoding,
        List[BatchEncoding],
        Dict[str, EncodedInput],
        Dict[str, List[EncodedInput]],
        List[Dict[str, EncodedInput]],
    ],
    padding: Union[bool, str, PaddingStrategy] = True,
    max_length: Optional[int] = None,
    pad_to_multiple_of: Optional[int] = None,
    return_attention_mask: Optional[bool] = None,
    return_tensors: Optional[Union[str, TensorType]] = None,
    verbose: bool = True,
) -&gt; BatchEncoding:

    # If we have a list of dicts, let&#39;s convert it in a dict of lists
    # We do this to allow using this method as a collate_fn function in PyTorch Dataloader
    if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], (dict, BatchEncoding)):
        encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}

    # The model&#39;s main input name, usually `input_ids`, has be passed for padding
    if self.model_input_names[0] not in encoded_inputs:
        raise ValueError(
            &#34;You should supply an encoding or a list of encodings to this method&#34;
            f&#34;that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}&#34;
        )

    required_input = encoded_inputs[self.model_input_names[0]]

    if required_input is None:
        if return_attention_mask:
            encoded_inputs[&#34;attention_mask&#34;] = []
        return encoded_inputs

    # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects
    # and rebuild them afterwards if no return_tensors is specified
    # Note that we lose the specific device the tensor may be on for PyTorch

    first_element = required_input[0]
    if isinstance(first_element, (list, tuple)):
        # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.
        index = 0
        while len(required_input[index]) == 0:
            index += 1
        if index &lt; len(required_input):
            first_element = required_input[index][0]
    # At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.
    if not isinstance(first_element, (int, list, tuple)):
        if is_torch_available() and _is_torch(first_element):
            return_tensors = &#34;pt&#34; if return_tensors is None else return_tensors
        elif isinstance(first_element, np.ndarray):
            return_tensors = &#34;np&#34; if return_tensors is None else return_tensors
        else:
            raise ValueError(
                f&#34;type of {first_element} unknown: {type(first_element)}. &#34;
                f&#34;Should be one of a python, numpy or pytorch object.&#34;
            )

        for key, value in encoded_inputs.items():
            encoded_inputs[key] = to_py_obj(value)
    
    required_input = encoded_inputs[self.model_input_names[0]]
    if required_input and not isinstance(required_input[0], (list, tuple)):
        encoded_inputs = self._pad(
            encoded_inputs,
            max_length=max_length,
            padding_strategy=padding,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )
        return BatchEncoding(encoded_inputs, tensor_type=return_tensors)

    batch_size = len(required_input)
    assert all(
        len(v) == batch_size for v in encoded_inputs.values()
    ), &#34;Some items in the output dictionary have a different batch size than others.&#34;

    if padding == PaddingStrategy.LONGEST:
        max_length = max(len(inputs) for inputs in required_input)
        padding = PaddingStrategy.MAX_LENGTH

    batch_outputs = {}
    for i in range(batch_size):
        inputs = dict((k, v[i]) for k, v in encoded_inputs.items())
        outputs = self._pad(
            inputs,
            max_length=max_length,
            padding_strategy=padding,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

        for key, value in outputs.items():
            if key not in batch_outputs:
                batch_outputs[key] = []
            batch_outputs[key].append(value)

    return BatchEncoding(batch_outputs, tensor_type=return_tensors)</code></pre>
</details>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.prepare_for_model"><code class="name flex">
<span>def <span class="ident">prepare_for_model</span></span>(<span>self, ids: List[int], pair_ids: Union[List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.file_utils.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.file_utils.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_length: bool = False, prepend_batch_axis: bool = False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
manages a moving window (with user defined stride) for overflowing tokens</p>
<h2 id="args">Args</h2>
<p>ids (:obj:<code>List[int]</code>):
Tokenized input ids of the first sequence. Can be obtained from a string by chaining the <code>tokenize</code>
and <code>convert_tokens_to_ids</code> methods.
pair_ids (:obj:<code>List[int]</code>, <code>optional</code>):
Tokenized input ids of the second sequence. Can be obtained from a string by chaining the <code>tokenize</code>
and <code>convert_tokens_to_ids</code> methods.</p>
<p>add_special_tokens (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>True</code>):
Whether or not to encode the sequences with the special tokens relative to their model.
padding (:obj:<code>bool</code>, :obj:<code>str</code> or :class:<code>~transformers.file_utils.PaddingStrategy</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Activates and controls padding. Accepts the following values:</p>
<pre><code>* :obj:&lt;code&gt;True&lt;/code&gt; or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a
  single sequence if provided).
* :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or to the
  maximum acceptable input length for the model if that argument is not provided.
* :obj:&lt;code&gt;False&lt;/code&gt; or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
  different lengths).
</code></pre>
<p>truncation (:obj:<code>bool</code>, :obj:<code>str</code> or :class:<code>~transformers.tokenization_utils_base.TruncationStrategy</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Activates and controls truncation. Accepts the following values:</p>
<pre><code>* :obj:&lt;code&gt;True&lt;/code&gt; or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument
  :obj:&lt;code&gt;max\_length&lt;/code&gt; or to the maximum acceptable input length for the model if that argument is not
  provided. This will truncate token by token, removing a token from the longest sequence in the pair
  if a pair of sequences (or a batch of pairs) is provided.
* :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or to
  the maximum acceptable input length for the model if that argument is not provided. This will only
  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
* :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:&lt;code&gt;max\_length&lt;/code&gt; or
  to the maximum acceptable input length for the model if that argument is not provided. This will only
  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
* :obj:&lt;code&gt;False&lt;/code&gt; or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with
  sequence lengths greater than the model maximum admissible input size).
</code></pre>
<p>max_length (:obj:<code>int</code>, <code>optional</code>):
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<pre><code>If left unset or set to :obj:&lt;code&gt;None&lt;/code&gt;, this will use the predefined model maximum length if a maximum
length is required by one of the truncation/padding parameters. If the model has no specific maximum
input length (like XLNet) truncation/padding to a maximum length will be deactivated.
</code></pre>
<p>stride (:obj:<code>int</code>, <code>optional</code>, defaults to 0):
If set to a number along with :obj:<code>max_length</code>, the overflowing tokens returned when
:obj:<code>return_overflowing_tokens=True</code> will contain some tokens from the end of the truncated sequence
returned to provide some overlap between truncated and overflowing sequences. The value of this
argument defines the number of overlapping tokens.
is_split_into_words (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not the input is already pre-tokenized (e.g., split into words), in which case the tokenizer
will skip the pre-tokenization step. This is useful for NER or token classification.
pad_to_multiple_of (:obj:<code>int</code>, <code>optional</code>):
If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).
return_tensors (:obj:<code>str</code> or :class:<code>~transformers.file_utils.TensorType</code>, <code>optional</code>):
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<pre><code>* :obj:`'tf'`: Return TensorFlow :obj:&lt;code&gt;tf.constant&lt;/code&gt; objects.
* :obj:`'pt'`: Return PyTorch :obj:&lt;code&gt;torch.Tensor&lt;/code&gt; objects.
* :obj:`'np'`: Return Numpy :obj:&lt;code&gt;np.ndarray&lt;/code&gt; objects.
</code></pre>
<p>return_token_type_ids (:obj:<code>bool</code>, <code>optional</code>):
Whether to return token type IDs. If left to the default, will return the token type IDs according to
the specific tokenizer's default, defined by the :obj:<code>return_outputs</code> attribute.</p>
<pre><code>`What are token type IDs? &lt;../glossary.html#token-type-ids&gt;`__
</code></pre>
<p>return_attention_mask (:obj:<code>bool</code>, <code>optional</code>):
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific tokenizer's default, defined by the :obj:<code>return_outputs</code> attribute.</p>
<pre><code>`What are attention masks? &lt;../glossary.html#attention-mask&gt;`__
</code></pre>
<p>return_overflowing_tokens (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not to return overflowing token sequences.
return_special_tokens_mask (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not to return special tokens mask information.
return_offsets_mapping (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):
Whether or not to return :obj:<code>(char_start, char_end)</code> for each token.</p>
<pre><code>This is only available on fast tokenizers inheriting from
:class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise
:obj:&lt;code&gt;NotImplementedError&lt;/code&gt;.
</code></pre>
<dl>
<dt>return_length
(:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>False</code>):</dt>
<dt>Whether or not to return the lengths of the encoded inputs.</dt>
<dt>verbose (:obj:<code>bool</code>, <code>optional</code>, defaults to :obj:<code>True</code>):</dt>
<dt>Whether or not to print more information and warnings.</dt>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>passed to the :obj:<code>self.tokenize()</code> method</dd>
</dl>
<h2 id="return">Return</h2>
<p>:class:<code>~transformers.BatchEncoding</code>: A :class:<code>~transformers.BatchEncoding</code> with the following fields:</p>
<ul>
<li><strong>input_ids</strong> &ndash; List of token ids to be fed to a model.</li>
</ul>
<p><code>What are input IDs? &lt;../glossary.html#input-ids&gt;</code>__</p>
<ul>
<li><strong>token_type_ids</strong> &ndash; List of token type ids to be fed to a model (when :obj:<code>return_token_type_ids=True</code>
or if <code>"token_type_ids"</code> is in :obj:<code>self.model_input_names</code>).</li>
</ul>
<p><code>What are token type IDs? &lt;../glossary.html#token-type-ids&gt;</code>__</p>
<ul>
<li><strong>attention_mask</strong> &ndash; List of indices specifying which tokens should be attended to by the model (when
:obj:<code>return_attention_mask=True</code> or if <code>"attention_mask"</code> is in :obj:<code>self.model_input_names</code>).</li>
</ul>
<p><code>What are attention masks? &lt;../glossary.html#attention-mask&gt;</code>__</p>
<ul>
<li><strong>overflowing_tokens</strong> &ndash; List of overflowing tokens sequences (when a :obj:<code>max_length</code> is specified and
:obj:<code>return_overflowing_tokens=True</code>).</li>
<li><strong>num_truncated_tokens</strong> &ndash; Number of tokens truncated (when a :obj:<code>max_length</code> is specified and
:obj:<code>return_overflowing_tokens=True</code>).</li>
<li><strong>special_tokens_mask</strong> &ndash; List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
regular sequence tokens (when :obj:<code>add_special_tokens=True</code> and :obj:<code>return_special_tokens_mask=True</code>).</li>
<li><strong>length</strong> &ndash; The length of the inputs (when :obj:<code>return_length=True</code>)</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_model(
    self,
    ids: List[int],
    pair_ids: Optional[List[int]] = None,
    add_special_tokens: bool = True,
    padding: Union[bool, str, PaddingStrategy] = False,
    truncation: Union[bool, str, TruncationStrategy] = False,
    max_length: Optional[int] = None,
    stride: int = 0,
    pad_to_multiple_of: Optional[int] = None,
    return_tensors: Optional[Union[str, TensorType]] = None,
    return_token_type_ids: Optional[bool] = None,
    return_attention_mask: Optional[bool] = None,
    return_overflowing_tokens: bool = False,
    return_special_tokens_mask: bool = False,
    return_length: bool = False,
    prepend_batch_axis: bool = False,
    **kwargs
):

    pair = bool(pair_ids is not None)
    len_ids = len(ids)
    len_pair_ids = len(pair_ids) if pair else 0

    if return_token_type_ids and not add_special_tokens:
        raise ValueError(
            &#34;Asking to return token_type_ids while setting add_special_tokens to False &#34;
            &#34;results in an undefined behavior. Please set add_special_tokens to True or &#34;
            &#34;set return_token_type_ids to None.&#34;
        )

    # Load from model defaults
    if return_token_type_ids is None:
        return_token_type_ids = &#34;token_type_ids&#34; in self.model_input_names
    if return_attention_mask is None:
        return_attention_mask = &#34;attention_mask&#34; in self.model_input_names

    encoded_inputs = {}

    # Compute the total size of the returned encodings
    total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)

    # Truncation: Handle max sequence length
    overflowing_tokens = []
    if truncation != TruncationStrategy.DO_NOT_TRUNCATE and max_length and total_len &gt; max_length:
        ids, pair_ids, overflowing_tokens = self.truncate_sequences(
            ids,
            pair_ids=pair_ids,
            num_tokens_to_remove=total_len - max_length,
            truncation_strategy=truncation,
            stride=stride,
        )

    if return_overflowing_tokens:
        encoded_inputs[&#34;overflowing_tokens&#34;] = overflowing_tokens
        encoded_inputs[&#34;num_truncated_tokens&#34;] = total_len - max_length

    # Add special tokens
    if add_special_tokens:
        sequence = self.build_inputs_with_special_tokens(ids, pair_ids)
        token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)
    else:
        sequence = np.concatenate([ids, pair_ids], axis=0) if pair is True else ids
        token_type_ids = [0] * len(ids) + ([0] * len(pair_ids) if pair else [])
    
    # Build output dictionary
    encoded_inputs[&#34;input_ids&#34;] = sequence

    if return_token_type_ids:
        encoded_inputs[&#34;token_type_ids&#34;] = token_type_ids
    if return_special_tokens_mask:
        if add_special_tokens:
            encoded_inputs[&#34;special_tokens_mask&#34;] = self.get_special_tokens_mask(ids, pair_ids)
        else:
            encoded_inputs[&#34;special_tokens_mask&#34;] = [0] * len(sequence)

    # Padding
    if padding != PaddingStrategy.DO_NOT_PAD or return_attention_mask:
        encoded_inputs = self.pad(
            encoded_inputs,
            max_length=max_length,
            padding=padding,
            pad_to_multiple_of=pad_to_multiple_of,
            return_attention_mask=return_attention_mask,
        )

    if return_length:
        encoded_inputs[&#34;length&#34;] = len(encoded_inputs[&#34;input_ids&#34;])

    
    batch_outputs = BatchEncoding(
        encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis
    )
    
    return batch_outputs</code></pre>
</details>
</dd>
<dt id="text_embeddings.hash.canine.CANINETokenizer.text2hashes"><code class="name flex">
<span>def <span class="ident">text2hashes</span></span>(<span>self, text: str) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Convert text into an numpy array, in (sequence_length, 1, hash_size) shape.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>Input text</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>An array in (sequence_length, 1, hash_size) shape</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text2hashes(self, text: str) -&gt; np.ndarray:
    &#34;&#34;&#34;Convert text into an numpy array, in (sequence_length, 1, hash_size) shape.

    Parameters
    ----------
    text : str
        Input text

    Returns
    -------
    np.ndarray
        An array in (sequence_length, 1, hash_size) shape
    &#34;&#34;&#34;
    if not text:
        return None
    
    result = np.zeros((len(text), self.hash_size))
    for i, char in enumerate(text):
        result[i] = murmurhash(char, feature_size=self.hash_size*2)
    
    return np.expand_dims(result, axis=1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="text_embeddings.hash" href="index.html">text_embeddings.hash</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="text_embeddings.hash.canine.CANINETokenizer" href="#text_embeddings.hash.canine.CANINETokenizer">CANINETokenizer</a></code></h4>
<ul class="">
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.build_inputs_with_special_tokens" href="#text_embeddings.hash.canine.CANINETokenizer.build_inputs_with_special_tokens">build_inputs_with_special_tokens</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.create_token_type_ids_from_sequences" href="#text_embeddings.hash.canine.CANINETokenizer.create_token_type_ids_from_sequences">create_token_type_ids_from_sequences</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.get_special_tokens_mask" href="#text_embeddings.hash.canine.CANINETokenizer.get_special_tokens_mask">get_special_tokens_mask</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.max_model_input_sizes" href="#text_embeddings.hash.canine.CANINETokenizer.max_model_input_sizes">max_model_input_sizes</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.model_input_names" href="#text_embeddings.hash.canine.CANINETokenizer.model_input_names">model_input_names</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.num_special_tokens_to_add" href="#text_embeddings.hash.canine.CANINETokenizer.num_special_tokens_to_add">num_special_tokens_to_add</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.pad" href="#text_embeddings.hash.canine.CANINETokenizer.pad">pad</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.padding_side" href="#text_embeddings.hash.canine.CANINETokenizer.padding_side">padding_side</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.prepare_for_model" href="#text_embeddings.hash.canine.CANINETokenizer.prepare_for_model">prepare_for_model</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.pretrained_init_configuration" href="#text_embeddings.hash.canine.CANINETokenizer.pretrained_init_configuration">pretrained_init_configuration</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.pretrained_vocab_files_map" href="#text_embeddings.hash.canine.CANINETokenizer.pretrained_vocab_files_map">pretrained_vocab_files_map</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.text2hashes" href="#text_embeddings.hash.canine.CANINETokenizer.text2hashes">text2hashes</a></code></li>
<li><code><a title="text_embeddings.hash.canine.CANINETokenizer.vocab_files_names" href="#text_embeddings.hash.canine.CANINETokenizer.vocab_files_names">vocab_files_names</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>